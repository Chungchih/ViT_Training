{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KcQPtWb-lDMm",
        "outputId": "bd1c6496-f361-47d4-c211-43d30f9618f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KcQPtWb-lDMm",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm==0.4.12\n",
        "!git clone https://github.com/Chungchih/ViT_Training.git\n",
        "%cd ViT_Training"
      ],
      "metadata": {
        "id": "1AmXP18Gbcm_",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04080bff-f235-41a1-c17a-74689adadc6b"
      },
      "id": "1AmXP18Gbcm_",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.10/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.2)\n",
            "fatal: destination path 'ViT_Training' already exists and is not an empty directory.\n",
            "/content/ViT_Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes==0.43.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NoDZbB8-1iB",
        "outputId": "fd8beb77-cd97-4986-8dff-aaaa0a5cc95c"
      },
      "id": "9NoDZbB8-1iB",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes==0.43.3 in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.3) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.3) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes==0.43.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.3) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "20c17525-abbf-4e53-9a8d-fb96e0dae40c",
      "metadata": {
        "id": "20c17525-abbf-4e53-9a8d-fb96e0dae40c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification, BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "import time\n",
        "import utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f5bdbb94-5654-44ac-a9eb-77794449cc90",
      "metadata": {
        "id": "f5bdbb94-5654-44ac-a9eb-77794449cc90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5ed49c-bdf0-41dc-e2e9-14998aac1ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Configure 8-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    #bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"facebook/deit-tiny-patch16-224\",  # example model name\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "# Load the feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    \"facebook/deit-tiny-patch16-224\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c11a6a4-3e7c-4ad6-ae58-863baa7761a3",
      "metadata": {
        "id": "2c11a6a4-3e7c-4ad6-ae58-863baa7761a3"
      },
      "outputs": [],
      "source": [
        "class NoiseQuantizedLinear8Bit(nn.Module):\n",
        "    def __init__(self, original_linear, x_noise_level=0.01, w_noise_level=0.01, quantization_bits=8):\n",
        "        super().__init__()\n",
        "        self.original_linear = original_linear\n",
        "        self.x_noise_level = x_noise_level\n",
        "        self.w_noise_level = w_noise_level\n",
        "        self.quantization_bits = quantization_bits\n",
        "\n",
        "    def add_quantization_noise(self, x, noise_level=0.01):\n",
        "        x_float = x.float()\n",
        "        # Generate noise\n",
        "        noise = torch.randn_like(x_float) * noise_level #* torch.std(x_float)\n",
        "\n",
        "        # Add noise and clip\n",
        "        noisy_x = x * (1+noise)\n",
        "        #noisy_x = torch.clamp(noisy_x, 0, 2**self.quantization_bits-1)  # Assume 8-bit quantization\n",
        "\n",
        "        return noisy_x\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        weights = self.original_linear.weight.data\n",
        "        weights_noisy = self.add_quantization_noise(weights, self.w_noise_level)\n",
        "        self.original_linear.weight.data = weights_noisy\n",
        "\n",
        "        # Add noise to quantized tensors\n",
        "        x_noisy = self.add_quantization_noise(x, self.x_noise_level)\n",
        "\n",
        "        # Perform linear transformation\n",
        "        output = self.original_linear(x_noisy.float())\n",
        "\n",
        "        #output = self.quantize_tensor(output, 16)\n",
        "        return output\n",
        "\n",
        "def add_noise_to_8bit_model(model, x_noise_level=0.01, w_noise_level=0.01, quantization_bits=8):\n",
        "    for name, module in model.named_children():\n",
        "\n",
        "        if hasattr(module, 'weight') and module.__class__.__name__ == 'Linear8bitLt':\n",
        "            #print(name)\n",
        "            setattr(model, name, NoiseQuantizedLinear8Bit(original_linear=module, x_noise_level=x_noise_level,\n",
        "                                    w_noise_level=w_noise_level, quantization_bits=8))\n",
        "        else:\n",
        "            add_noise_to_8bit_model(module, x_noise_level, w_noise_level, quantization_bits)\n",
        "    return model\n",
        "\n",
        "#model = add_noise_to_8bit_model(model, 0.1, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.data import create_transform\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def imgFolder_mapping(data_path,):\n",
        "    folder_classes = sorted(os.listdir(os.path.join(data_path, 'train')))\n",
        "    folder_to_model_idx = {}\n",
        "    original_classes_names = np.genfromtxt(r'/content/ViT_Training/LOC_synset_mapping.txt', delimiter='!', dtype='str')\n",
        "    original_classes_names = list(map(lambda x: x[0],map(str.split,original_classes_names)))\n",
        "    for idx, name in enumerate(folder_classes):\n",
        "        original_idx = original_classes_names.index(name)\n",
        "        folder_to_model_idx[idx] = original_idx\n",
        "    return folder_to_model_idx\n",
        "\n",
        "def trans_idx(idx):\n",
        "    folder_to_model_idx = imgFolder_mapping(r'/content/drive/MyDrive/mini_imagenet')\n",
        "    return folder_to_model_idx[idx]\n",
        "\n",
        "def build_transform(is_train=False):\n",
        "    resize_im = 224 > 32\n",
        "    if is_train:\n",
        "        # this should always dispatch to transforms_imagenet_train\n",
        "        transform = create_transform(\n",
        "            input_size=224,\n",
        "            is_training=True,\n",
        "            color_jitter=0.3,\n",
        "            auto_augment='rand-m9-mstd0.5-inc1',\n",
        "            interpolation='bicubic',\n",
        "            re_prob=0.25,\n",
        "            re_mode='pixel',\n",
        "            re_count=1,\n",
        "        )\n",
        "        if not resize_im:\n",
        "            # replace RandomResizedCropAndInterpolation with\n",
        "            # RandomCrop\n",
        "            transform.transforms[0] = transforms.RandomCrop(\n",
        "                224, padding=4)\n",
        "        return transform\n",
        "\n",
        "    t = []\n",
        "    if True:\n",
        "        size = int(224 / 0.875)\n",
        "        t.append(\n",
        "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
        "        )\n",
        "        t.append(transforms.CenterCrop(224))\n",
        "\n",
        "    t.append(transforms.ToTensor())\n",
        "    #t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
        "    t.append(transforms.Normalize(feature_extractor.image_mean, feature_extractor.image_std))\n",
        "    return transforms.Compose(t)\n",
        "\n",
        "def build_dataset(is_train, path):\n",
        "    transform = build_transform(is_train,)\n",
        "\n",
        "    root = os.path.join(path, 'train' if is_train else 'val')\n",
        "    dataset = datasets.ImageFolder(root, transform=transform, target_transform=trans_idx)\n",
        "    nb_classes = 1000\n",
        "\n",
        "    return dataset, nb_classes\n",
        "\n",
        "#path = 'C:/Computing/mini_imagenet'\n",
        "#path = r'C:/Computing/zhuhanqing-Lightening-Transformer-25e9859/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/'\n",
        "path = '/content/drive/MyDrive/mini_imagenet'\n",
        "\n",
        "dataset_train, nb_classes = build_dataset(is_train=1, path=path)\n",
        "dataset_val, _ = build_dataset(is_train=0, path=path)\n",
        "\n",
        "sampler_val = torch.utils.data.SequentialSampler(dataset_val)"
      ],
      "metadata": {
        "id": "YaQqq4dhJu7k"
      },
      "id": "YaQqq4dhJu7k",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "473b2aa1-f362-4acf-9280-beada3971ba9",
      "metadata": {
        "id": "473b2aa1-f362-4acf-9280-beada3971ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a7309d-264c-4f68-db2d-9870fdada2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:  [ 0/24]  eta: 0:46:53  loss: 1.0992 (1.0992)  acc1: 91.7969 (91.7969)  acc5: 98.0469 (98.0469)  time: 117.2112  data: 106.9824  max mem: 1911\n",
            "Test:  [10/24]  eta: 0:03:56  loss: 1.6106 (1.5797)  acc1: 75.7812 (77.9652)  acc5: 94.3359 (93.4837)  time: 16.9214  data: 13.9041  max mem: 1934\n",
            "Test:  [20/24]  eta: 0:00:47  loss: 1.7441 (1.6776)  acc1: 73.2422 (74.8419)  acc5: 91.4062 (92.2061)  time: 6.7185  data: 4.5620  max mem: 1934\n",
            "Test:  [23/24]  eta: 0:00:10  loss: 1.7441 (1.6522)  acc1: 73.2422 (75.3500)  acc5: 91.4062 (92.6333)  time: 6.5042  data: 4.5597  max mem: 1934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: Total time: 0:04:15 (10.6385 s / it)\n",
            "* Acc@1 75.350 Acc@5 92.633 loss 1.652\n",
            "Test:  [ 0/24]  eta: 0:32:56  loss: 1.0992 (1.0992)  acc1: 91.7969 (91.7969)  acc5: 98.0469 (98.0469)  time: 82.3531  data: 79.8022  max mem: 1935\n",
            "Test:  [10/24]  eta: 0:03:38  loss: 1.6106 (1.5797)  acc1: 75.7812 (77.9652)  acc5: 94.3359 (93.4837)  time: 15.6115  data: 13.5312  max mem: 1958\n",
            "Test:  [20/24]  eta: 0:00:45  loss: 1.7441 (1.6776)  acc1: 73.2422 (74.8419)  acc5: 91.4062 (92.2061)  time: 7.9334  data: 5.8882  max mem: 1958\n",
            "Test:  [23/24]  eta: 0:00:10  loss: 1.7441 (1.6522)  acc1: 73.2422 (75.3500)  acc5: 91.4062 (92.6333)  time: 7.7137  data: 5.8881  max mem: 1958\n",
            "Test: Total time: 0:04:04 (10.2071 s / it)\n",
            "* Acc@1 75.350 Acc@5 92.633 loss 1.652\n"
          ]
        }
      ],
      "source": [
        "from timm.utils import accuracy\n",
        "def evaluate(data_loader_val, model, device):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    num_images = 0\n",
        "    for images, target in metric_logger.log_every(data_loader_val, 10, header):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "        num_images += images.shape[0]\n",
        "\n",
        "        # compute output\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            with torch.no_grad():\n",
        "                output = model(images).logits\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "        metric_logger.update(loss=loss.item())\n",
        "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
        "        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
        "          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n",
        "\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "from samplers import RASampler\n",
        "num_tasks = utils.get_world_size()\n",
        "global_rank = utils.get_rank()\n",
        "\n",
        "sampler_train = RASampler(\n",
        "    dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
        ")\n",
        "\n",
        "data_loader_val = torch.utils.data.DataLoader(\n",
        "    dataset_val, sampler=sampler_val,\n",
        "    batch_size=int(512),\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        "\n",
        ")\n",
        "device = 'cuda'\n",
        "'''\n",
        "x_noise_level = 0.1\n",
        "w_noise_level = 0.1\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"facebook/deit-tiny-patch16-224\",  # example model name\n",
        "    quantization_config=quantization_config)\n",
        "\n",
        "model = add_noise_to_8bit_model(model=model, x_noise_level=x_noise_level, w_noise_level=w_noise_level, quantization_bits=8)\n",
        "\n",
        "resevaluate(data_loader_val, model, device='cuda')\n",
        "'''\n",
        "results = []\n",
        "output_dir = '/content/drive/MyDrive/mini_imagenet/finetune'\n",
        "\n",
        "import json\n",
        "for x_noise in np.linspace(0,0,1):\n",
        "  for w_noise in np.linspace(0.,0.3,2):\n",
        "    model = AutoModelForImageClassification.from_pretrained(\"facebook/deit-tiny-patch16-224\", quantization_config=quantization_config)\n",
        "    model = add_noise_to_8bit_model(model, x_noise_level=x_noise, w_noise_level=w_noise, quantization_bits=8)\n",
        "    result = evaluate(data_loader_val, model, device='cuda')\n",
        "    results.append(result)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    with open('/content/drive/MyDrive/mini_imagenet/finetune/q8_log.txt','a') as f:\n",
        "      f.write(json.dumps(result)+'\\n')\n",
        "\n",
        "with open('/content/drive/MyDrive/mini_imagenet/finetune/q8_log_all.txt','a') as f:\n",
        "  f.write(json.dumps(results)+'\\n')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "pKCtOvnjk-WB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b32dba8-d311-4938-d272-daad045ea266"
      },
      "id": "pKCtOvnjk-WB",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec  3 08:25:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0              35W /  70W |    405MiB / 15360MiB |      2%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}