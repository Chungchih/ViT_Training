{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KcQPtWb-lDMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda4a848-a1d3-4fb2-8c51-88a6b6b3558d"
      },
      "id": "KcQPtWb-lDMm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm==0.4.12\n",
        "!git clone https://github.com/Chungchih/ViT_Training.git\n",
        "%cd ViT_Training"
      ],
      "metadata": {
        "id": "1AmXP18Gbcm_",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad42639-1eb3-48ec-8058-03451bafca21"
      },
      "id": "1AmXP18Gbcm_",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.10/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.2)\n",
            "fatal: destination path 'ViT_Training' already exists and is not an empty directory.\n",
            "/content/ViT_Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "20c17525-abbf-4e53-9a8d-fb96e0dae40c",
      "metadata": {
        "id": "20c17525-abbf-4e53-9a8d-fb96e0dae40c"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import json\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from timm.data import Mixup\n",
        "from timm.models import create_model\n",
        "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
        "from timm.scheduler import create_scheduler\n",
        "from timm.optim import create_optimizer\n",
        "from timm.utils import NativeScaler, get_state_dict, ModelEma\n",
        "\n",
        "from datasets_mini import build_dataset, build_dataset_preload\n",
        "from engine import train_one_epoch, evaluate\n",
        "from losses import DistillationLoss\n",
        "from samplers import RASampler\n",
        "from augment import new_data_aug_generator\n",
        "\n",
        "import models\n",
        "import models_v2\n",
        "from quant_model import quant_vit\n",
        "\n",
        "import utils\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f5bdbb94-5654-44ac-a9eb-77794449cc90",
      "metadata": {
        "id": "f5bdbb94-5654-44ac-a9eb-77794449cc90"
      },
      "outputs": [],
      "source": [
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('DeiT training and evaluation script', add_help=False)\n",
        "    parser.add_argument('--batch-size', default=32, type=int)\n",
        "    parser.add_argument('--epochs', default=300, type=int)\n",
        "    parser.add_argument('--bce-loss', action='store_true')\n",
        "    parser.add_argument('--unscale-lr', action='store_true')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--model', default='deit_base_patch16_224', type=str, metavar='MODEL',\n",
        "                        help='Name of model to train')\n",
        "    parser.add_argument('--input-size', default=224, type=int, help='images input size')\n",
        "\n",
        "    parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
        "                        help='Dropout rate (default: 0.)')\n",
        "    parser.add_argument('--drop-path', type=float, default=0.1, metavar='PCT',\n",
        "                        help='Drop path rate (default: 0.1)')\n",
        "\n",
        "    parser.add_argument('--model-ema', action='store_true')\n",
        "    parser.add_argument('--no-model-ema', action='store_false', dest='model_ema')\n",
        "    parser.set_defaults(model_ema=True)\n",
        "    parser.add_argument('--model-ema-decay', type=float, default=0.99996, help='')\n",
        "    parser.add_argument('--model-ema-force-cpu', action='store_true', default=False, help='')\n",
        "\n",
        "    # Optimizer parameters\n",
        "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
        "                        help='Optimizer (default: \"adamw\"')\n",
        "    parser.add_argument('--opt-eps', default=1e-8, type=float, metavar='EPSILON',\n",
        "                        help='Optimizer Epsilon (default: 1e-8)')\n",
        "    parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
        "                        help='Optimizer Betas (default: None, use opt default)')\n",
        "    parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
        "                        help='Clip gradient norm (default: None, no clipping)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                        help='SGD momentum (default: 0.9)')\n",
        "    parser.add_argument('--weight-decay', type=float, default=0.05,\n",
        "                        help='weight decay (default: 0.05)')\n",
        "    # Learning rate schedule parameters\n",
        "    parser.add_argument('--sched', default='cosine', type=str, metavar='SCHEDULER',\n",
        "                        help='LR scheduler (default: \"cosine\"')\n",
        "    parser.add_argument('--lr', type=float, default=5e-4, metavar='LR',\n",
        "                        help='learning rate (default: 5e-4)')\n",
        "    parser.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
        "                        help='learning rate noise on/off epoch percentages')\n",
        "    parser.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
        "                        help='learning rate noise limit percent (default: 0.67)')\n",
        "    parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
        "                        help='learning rate noise std-dev (default: 1.0)')\n",
        "    parser.add_argument('--warmup-lr', type=float, default=1e-6, metavar='LR',\n",
        "                        help='warmup learning rate (default: 1e-6)')\n",
        "    parser.add_argument('--min-lr', type=float, default=1e-5, metavar='LR',\n",
        "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
        "\n",
        "    parser.add_argument('--decay-epochs', type=float, default=30, metavar='N',\n",
        "                        help='epoch interval to decay LR')\n",
        "    parser.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n",
        "                        help='epochs to warmup LR, if scheduler supports')\n",
        "    parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='N',\n",
        "                        help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
        "    parser.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
        "                        help='patience epochs for Plateau LR scheduler (default: 10')\n",
        "    parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
        "                        help='LR decay rate (default: 0.1)')\n",
        "\n",
        "    # Augmentation parameters\n",
        "    parser.add_argument('--color-jitter', type=float, default=0.3, metavar='PCT',\n",
        "                        help='Color jitter factor (default: 0.3)')\n",
        "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
        "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \\\n",
        "                             \"(default: rand-m9-mstd0.5-inc1)'),\n",
        "    parser.add_argument('--smoothing', type=float, default=0.1, help='Label smoothing (default: 0.1)')\n",
        "    parser.add_argument('--train-interpolation', type=str, default='bicubic',\n",
        "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
        "\n",
        "    parser.add_argument('--repeated-aug', action='store_true')\n",
        "    parser.add_argument('--no-repeated-aug', action='store_false', dest='repeated_aug')\n",
        "    parser.set_defaults(repeated_aug=True)\n",
        "\n",
        "    parser.add_argument('--train-mode', action='store_true')\n",
        "    parser.add_argument('--no-train-mode', action='store_false', dest='train_mode')\n",
        "    parser.set_defaults(train_mode=True)\n",
        "\n",
        "    parser.add_argument('--ThreeAugment', action='store_true') #3augment\n",
        "\n",
        "    parser.add_argument('--src', action='store_true') #simple random crop\n",
        "\n",
        "    # * Random Erase params\n",
        "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
        "                        help='Random erase prob (default: 0.25)')\n",
        "    parser.add_argument('--remode', type=str, default='pixel',\n",
        "                        help='Random erase mode (default: \"pixel\")')\n",
        "    parser.add_argument('--recount', type=int, default=1,\n",
        "                        help='Random erase count (default: 1)')\n",
        "    parser.add_argument('--resplit', action='store_true', default=False,\n",
        "                        help='Do not random erase first (clean) augmentation split')\n",
        "\n",
        "    # * Mixup params\n",
        "    parser.add_argument('--mixup', type=float, default=0.8,\n",
        "                        help='mixup alpha, mixup enabled if > 0. (default: 0.8)')\n",
        "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
        "                        help='cutmix alpha, cutmix enabled if > 0. (default: 1.0)')\n",
        "    parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
        "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
        "    parser.add_argument('--mixup-prob', type=float, default=1.0,\n",
        "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
        "    parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
        "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
        "    parser.add_argument('--mixup-mode', type=str, default='batch',\n",
        "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
        "\n",
        "    # Distillation parameters\n",
        "    parser.add_argument('--teacher-model', default='regnety_160', type=str, metavar='MODEL',\n",
        "                        help='Name of teacher model to train (default: \"regnety_160\"')\n",
        "    parser.add_argument('--teacher-path', type=str, default='')\n",
        "    parser.add_argument('--distillation-type', default='none', choices=['none', 'soft', 'hard'], type=str, help=\"\")\n",
        "    parser.add_argument('--distillation-alpha', default=0.5, type=float, help=\"\")\n",
        "    parser.add_argument('--distillation-tau', default=1.0, type=float, help=\"\")\n",
        "\n",
        "    # * Finetuning params\n",
        "    parser.add_argument('--finetune', default='', help='finetune from checkpoint')\n",
        "    parser.add_argument('--attn-only', action='store_true')\n",
        "\n",
        "    # Dataset parameters\n",
        "    parser.add_argument('--data-path', default=r'./data/', type=str,\n",
        "                        help='dataset path')\n",
        "    parser.add_argument('--data-set', default='IMNET', choices=['CIFAR', 'IMNET', 'INAT', 'INAT19'],\n",
        "                        type=str, help='Image Net dataset path')\n",
        "    parser.add_argument('--inat-category', default='name',\n",
        "                        choices=['kingdom', 'phylum', 'class', 'order', 'supercategory', 'family', 'genus', 'name'],\n",
        "                        type=str, help='semantic granularity')\n",
        "\n",
        "    parser.add_argument('--output_dir', default='',\n",
        "                        help='path where to save, empty for no saving')\n",
        "    parser.add_argument('--device', default='cuda',\n",
        "                        help='device to use for training / testing')\n",
        "    parser.add_argument('--seed', default=0, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
        "                        help='start epoch')\n",
        "    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n",
        "    parser.add_argument('--eval-crop-ratio', default=0.875, type=float, help=\"Crop ratio for evaluation\")\n",
        "    parser.add_argument('--dist-eval', action='store_true', default=False, help='Enabling distributed evaluation')\n",
        "    parser.add_argument('--num_workers', default=8, type=int)\n",
        "    parser.add_argument('--pin-mem', action='store_true',\n",
        "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
        "    parser.add_argument('--no-pin-mem', action='store_false', dest='pin_mem',\n",
        "                        help='')\n",
        "    parser.set_defaults(pin_mem=True)\n",
        "\n",
        "    # distributed training parameters\n",
        "    parser.add_argument('--world_size', default=1, type=int,\n",
        "                        help='number of distributed processes')\n",
        "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
        "\n",
        "\n",
        "    # sparse mask parameters\n",
        "    parser.add_argument('--apply_mask', action='store_true', help='Whether apply pre-stored sparse mask')\n",
        "    parser.add_argument('--mask_path', default='./logs/attn_thres-0.005.txt', type=str,\n",
        "                        help='dataset path')\n",
        "\n",
        "    # quantization parameters\n",
        "    parser.add_argument('--wbits', default=-1, type=int)\n",
        "    parser.add_argument('--abits', default=-1, type=int)\n",
        "    parser.add_argument('--headwise', action='store_true', default=False)\n",
        "    parser.add_argument('--offset', action='store_true', default=False)\n",
        "\n",
        "    # noise parameters\n",
        "    parser.add_argument('--input_noise_std', default=0, type=float)\n",
        "    parser.add_argument('--output_noise_std', default=0, type=float)\n",
        "    parser.add_argument('--phase_noise_std', default=0, type=float)\n",
        "    parser.add_argument('--enable_wdm_noise', action='store_true', help='Whether enable wdm noise for coupler')\n",
        "    parser.add_argument('--enable_linear_noise', action='store_true', help='Whether enable noise for linear')\n",
        "    parser.add_argument('--num_wavelength', default=9, type=int)\n",
        "    parser.add_argument('--channel_spacing', default=0.4, type=float)\n",
        "    parser.add_argument('--save_vit_params', action='store_true', help='Whether save_vit_params')\n",
        "    parser.add_argument('--enable_calibration', action='store_true', help='Whether calibration')\n",
        "    parser.add_argument('--restart_finetune', action='store_true', help='Whether restart fine-tune')\n",
        "\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2c11a6a4-3e7c-4ad6-ae58-863baa7761a3",
      "metadata": {
        "id": "2c11a6a4-3e7c-4ad6-ae58-863baa7761a3"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    utils.init_distributed_mode(args)\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    if args.distillation_type != 'none' and args.finetune and not args.eval:\n",
        "        raise NotImplementedError(\"Finetuning with distillation not yet supported\")\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    seed = args.seed + utils.get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    print(seed)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)\n",
        "    #dataset_train, args.nb_classes = build_dataset_preload(is_train=True, args=args), 1000\n",
        "    dataset_val, _ = build_dataset(is_train=False, args=args)\n",
        "\n",
        "    if True:  # args.distributed:\n",
        "        num_tasks = utils.get_world_size()\n",
        "        global_rank = utils.get_rank()\n",
        "        if args.repeated_aug:\n",
        "            sampler_train = RASampler(\n",
        "                dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
        "            )\n",
        "        else:\n",
        "            sampler_train = torch.utils.data.DistributedSampler(\n",
        "                dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
        "            )\n",
        "        if args.dist_eval:\n",
        "            if len(dataset_val) % num_tasks != 0:\n",
        "                print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n",
        "                      'This will slightly alter validation results as extra duplicate entries are added to achieve '\n",
        "                      'equal num of samples per-process.')\n",
        "            sampler_val = torch.utils.data.DistributedSampler(\n",
        "                dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=False)\n",
        "        else:\n",
        "            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "    data_loader_train = torch.utils.data.DataLoader(\n",
        "        dataset_train, sampler=sampler_train,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=args.pin_mem,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    if args.ThreeAugment:\n",
        "        data_loader_train.dataset.transform = new_data_aug_generator(args)\n",
        "\n",
        "    # prepare data\n",
        "    data_loader_val = torch.utils.data.DataLoader(\n",
        "        dataset_val, sampler=sampler_val,\n",
        "        batch_size=int(1.5 * args.batch_size),\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=args.pin_mem,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    mixup_fn = None\n",
        "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
        "    if mixup_active:\n",
        "        mixup_fn = Mixup(\n",
        "            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n",
        "            prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n",
        "            label_smoothing=args.smoothing, num_classes=args.nb_classes)\n",
        "\n",
        "    # prepare model\n",
        "    print(f\"Creating model: {args.model}\")\n",
        "    print(f\"Noise param:\")\n",
        "    print(f\"** Enable linear noise {args.enable_linear_noise}\")\n",
        "    print(f\"** Input noise std {args.input_noise_std}\")\n",
        "    print(f\"** Output noise std {args.output_noise_std}\")\n",
        "    print(f\"** Phase noise std {args.phase_noise_std}\")\n",
        "    print(f\"** Enable WDM noise for coupler {args.enable_wdm_noise}\")\n",
        "    print(f\"** Num of wavelength {args.num_wavelength}\")\n",
        "    print(f\"** Channel spacing {args.channel_spacing}\")\n",
        "\n",
        "    ## Note that the noise std is assumed to be 2sigma value, so we need to divide by 2 before we generate it\n",
        "    if 'quant' in args.model:\n",
        "      model = create_model(\n",
        "          args.model,\n",
        "          pretrained=False,\n",
        "          num_classes=args.nb_classes,\n",
        "          drop_rate=args.drop,\n",
        "          drop_path_rate=args.drop_path,\n",
        "          drop_block_rate=None,\n",
        "          img_size=args.input_size,\n",
        "          wbits=args.wbits,\n",
        "          abits=args.abits,\n",
        "          headwise=args.headwise,\n",
        "          offset = args.offset,\n",
        "          input_noise_std=args.input_noise_std/2,\n",
        "          output_noise_std=args.output_noise_std/2,\n",
        "          phase_noise_std=args.phase_noise_std/2,\n",
        "          enable_wdm_noise=args.enable_wdm_noise,\n",
        "          enable_linear_noise=args.enable_linear_noise,\n",
        "          num_wavelength=args.num_wavelength,\n",
        "          channel_spacing=args.channel_spacing,\n",
        "          pretrained_cfg=None,\n",
        "          pretrained_cfg_overlay=None,\n",
        "      )\n",
        "    else:\n",
        "      model = create_model(\n",
        "          args.model,\n",
        "          pretrained=False,\n",
        "          num_classes=args.nb_classes,\n",
        "          drop_rate=args.drop,\n",
        "          drop_path_rate=args.drop_path,\n",
        "          drop_block_rate=None,\n",
        "          img_size=args.input_size,\n",
        "      )\n",
        "\n",
        "\n",
        "    if args.finetune:\n",
        "        if args.finetune.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.finetune, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.finetune, map_location='cpu')\n",
        "\n",
        "        checkpoint_model = checkpoint['model']\n",
        "        state_dict = model.state_dict()\n",
        "        for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
        "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "                del checkpoint_model[k]\n",
        "\n",
        "        # interpolate position embedding\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "        # only the position tokens are interpolated\n",
        "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "        pos_tokens = torch.nn.functional.interpolate(\n",
        "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "        checkpoint_model['pos_embed'] = new_pos_embed\n",
        "\n",
        "        model.load_state_dict(checkpoint_model, strict=False)\n",
        "        print(f'Finish loading pretrained model')\n",
        "\n",
        "    if args.attn_only:\n",
        "        for name_p,p in model.named_parameters():\n",
        "            if '.attn.' in name_p:\n",
        "                p.requires_grad = True\n",
        "            else:\n",
        "                p.requires_grad = False\n",
        "        try:\n",
        "            model.head.weight.requires_grad = True\n",
        "            model.head.bias.requires_grad = True\n",
        "        except:\n",
        "            model.fc.weight.requires_grad = True\n",
        "            model.fc.bias.requires_grad = True\n",
        "        try:\n",
        "            model.pos_embed.requires_grad = True\n",
        "        except:\n",
        "            print('no position encoding')\n",
        "        try:\n",
        "            for p in model.patch_embed.parameters():\n",
        "                p.requires_grad = False\n",
        "        except:\n",
        "            print('no patch embed')\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model_ema = None\n",
        "    if args.model_ema:\n",
        "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n",
        "        model_ema = ModelEma(\n",
        "            model,\n",
        "            decay=args.model_ema_decay,\n",
        "            device='cpu' if args.model_ema_force_cpu else '',\n",
        "            resume='')\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params:', n_parameters)\n",
        "    if not args.unscale_lr:\n",
        "        linear_scaled_lr = args.lr * args.batch_size * utils.get_world_size() / 512.0\n",
        "        args.lr = linear_scaled_lr\n",
        "    optimizer = create_optimizer(args, model_without_ddp)\n",
        "    loss_scaler = NativeScaler()\n",
        "\n",
        "    lr_scheduler, _ = create_scheduler(args, optimizer)\n",
        "\n",
        "    criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "    if mixup_active:\n",
        "        # smoothing is handled with mixup label transform\n",
        "        criterion = SoftTargetCrossEntropy()\n",
        "    elif args.smoothing:\n",
        "        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
        "    else:\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    if args.bce_loss:\n",
        "        criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    teacher_model = None\n",
        "    if args.distillation_type != 'none':\n",
        "        assert args.teacher_path, 'need to specify teacher-path when using distillation'\n",
        "        print(f\"Creating teacher model: {args.teacher_model}\")\n",
        "        teacher_model = create_model(\n",
        "            args.teacher_model,\n",
        "            pretrained=False,\n",
        "            num_classes=args.nb_classes,\n",
        "            global_pool='avg',\n",
        "        )\n",
        "        if args.teacher_path.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.teacher_path, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.teacher_path, map_location='cpu')\n",
        "        teacher_model.load_state_dict(checkpoint['model'])\n",
        "        teacher_model.to(device)\n",
        "        teacher_model.eval()\n",
        "\n",
        "    # wrap the criterion in our custom DistillationLoss, which\n",
        "    # just dispatches to the original criterion if args.distillation_type is 'none'\n",
        "    criterion = DistillationLoss(\n",
        "        criterion, teacher_model, args.distillation_type, args.distillation_alpha, args.distillation_tau\n",
        "    )\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    output_dir = Path(args.output_dir)\n",
        "    if args.resume:\n",
        "        if args.resume.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.resume, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        # print(checkpoint[\"model\"].keys())\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "\n",
        "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint and not args.restart_finetune:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "            args.start_epoch = checkpoint['epoch'] + 1\n",
        "            if args.model_ema:\n",
        "                utils._load_checkpoint_for_ema(model_ema, checkpoint['model_ema'])\n",
        "            if 'scaler' in checkpoint:\n",
        "                loss_scaler.load_state_dict(checkpoint['scaler'])\n",
        "        lr_scheduler.step(args.start_epoch)\n",
        "\n",
        "    if args.eval:\n",
        "        test_stats, total_loss = evaluate(data_loader_val, model, device)\n",
        "        # test_stats = evaluate(data_loader_val, model, device)\n",
        "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "        return\n",
        "\n",
        "    print(f\"Start training for {args.epochs} epochs\")\n",
        "    start_time = time.time()\n",
        "    max_accuracy = 0.0\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            data_loader_train.sampler.set_epoch(epoch)\n",
        "\n",
        "        train_stats = train_one_epoch(\n",
        "            model, criterion, data_loader_train,\n",
        "            optimizer, device, epoch, loss_scaler,\n",
        "            args.clip_grad, model_ema, mixup_fn,\n",
        "            set_training_mode=args.train_mode,  # keep in eval mode for deit finetuning / train mode for training and deit III finetuning\n",
        "            args = args,\n",
        "        )\n",
        "\n",
        "        lr_scheduler.step(epoch)\n",
        "        if args.output_dir:\n",
        "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "            for checkpoint_path in checkpoint_paths:\n",
        "                utils.save_on_master({\n",
        "                    'model': model_without_ddp.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'model_ema': get_state_dict(model_ema),\n",
        "                    'scaler': loss_scaler.state_dict(),\n",
        "                    'args': args,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "\n",
        "        test_stats, total_loss = evaluate(data_loader_val, model, device)\n",
        "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "        if max_accuracy < test_stats[\"acc1\"]:\n",
        "            max_accuracy = test_stats[\"acc1\"]\n",
        "            if args.output_dir:\n",
        "                checkpoint_paths = [output_dir / 'best_checkpoint.pth']\n",
        "                for checkpoint_path in checkpoint_paths:\n",
        "                    utils.save_on_master({\n",
        "                        'model': model_without_ddp.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'model_ema': get_state_dict(model_ema),\n",
        "                        'scaler': loss_scaler.state_dict(),\n",
        "                        'args': args,\n",
        "                    }, checkpoint_path)\n",
        "\n",
        "        print(f'Max accuracy: {max_accuracy:.2f}%')\n",
        "\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                     'epoch': epoch,\n",
        "                     'n_parameters': n_parameters}\n",
        "\n",
        "        if args.output_dir and utils.is_main_process():\n",
        "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "473b2aa1-f362-4acf-9280-beada3971ba9",
      "metadata": {
        "id": "473b2aa1-f362-4acf-9280-beada3971ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d16f90ed-1240-4228-8960-7220b6059eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(batch_size=300, epochs=300, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224_quant', input_size=224, drop=0.0, drop_path=0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=1e-08, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=0, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='/content/drive/MyDrive/mini_imagenet/resumed_ckpt/best_checkpoint_int8.pth', attn_only=False, data_path='/content/drive/MyDrive/mini_imagenet', data_set='IMNET', inat_category='name', output_dir='/content/drive/MyDrive/mini_imagenet/finetune', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=0, num_workers=0, pin_mem=True, world_size=1, dist_url='env://', apply_mask=False, mask_path='./logs/attn_thres-0.005.txt', wbits=8, abits=8, headwise=True, offset=False, input_noise_std=0.0, output_noise_std=0.0, phase_noise_std=0, enable_wdm_noise=False, enable_linear_noise=True, num_wavelength=1, channel_spacing=0.4, save_vit_params=False, enable_calibration=False, restart_finetune=False, distributed=False)\n",
            "0\n",
            "Creating model: deit_tiny_patch16_224_quant\n",
            "Noise param:\n",
            "** Enable linear noise True\n",
            "** Input noise std 0.0\n",
            "** Output noise std 0.0\n",
            "** Phase noise std 0\n",
            "** Enable WDM noise for coupler False\n",
            "** Num of wavelength 1\n",
            "** Channel spacing 0.4\n",
            "Use 8 bit weights.\n",
            "Use 8 bit activations.\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n",
            "Linear layer (mode: Qmodes.layer_wise): initialize weight scale for int8 quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-95317eee6636>:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.finetune, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish loading pretrained model\n",
            "number of params: 5717696\n",
            "Start training for 300 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "/content/ViT_Training/engine.py:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/159]  eta: 0:17:40  lr: 0.000029  loss: 1.9831 (1.9831)  time: 6.6721  data: 5.1659  max mem: 29130\n",
            "Epoch: [0]  [ 10/159]  eta: 0:13:26  lr: 0.000029  loss: 1.7329 (1.7505)  time: 5.4140  data: 4.7948  max mem: 29171\n",
            "Epoch: [0]  [ 20/159]  eta: 0:12:11  lr: 0.000029  loss: 1.7135 (1.7531)  time: 5.1930  data: 4.6642  max mem: 29171\n",
            "Epoch: [0]  [ 30/159]  eta: 0:11:12  lr: 0.000029  loss: 1.7135 (1.7366)  time: 5.1005  data: 4.5749  max mem: 29171\n",
            "Epoch: [0]  [ 40/159]  eta: 0:10:15  lr: 0.000029  loss: 1.6664 (1.7116)  time: 5.0755  data: 4.5500  max mem: 29171\n",
            "Epoch: [0]  [ 50/159]  eta: 0:09:18  lr: 0.000029  loss: 1.5878 (1.6835)  time: 4.9838  data: 4.4583  max mem: 29171\n",
            "Epoch: [0]  [ 60/159]  eta: 0:08:25  lr: 0.000029  loss: 1.5739 (1.6537)  time: 4.9708  data: 4.4451  max mem: 29171\n",
            "Epoch: [0]  [ 70/159]  eta: 0:07:34  lr: 0.000029  loss: 1.5325 (1.6432)  time: 5.0628  data: 4.5367  max mem: 29171\n",
            "Epoch: [0]  [ 80/159]  eta: 0:06:42  lr: 0.000029  loss: 1.5026 (1.6193)  time: 5.0412  data: 4.5157  max mem: 29171\n",
            "Epoch: [0]  [ 90/159]  eta: 0:07:13  lr: 0.000029  loss: 1.4828 (1.6079)  time: 10.4662  data: 9.9398  max mem: 29171\n",
            "Epoch: [0]  [100/159]  eta: 0:08:15  lr: 0.000029  loss: 1.4329 (1.5921)  time: 21.7761  data: 21.2490  max mem: 29171\n",
            "Epoch: [0]  [110/159]  eta: 0:08:14  lr: 0.000029  loss: 1.4205 (1.5723)  time: 27.4207  data: 26.8934  max mem: 29171\n",
            "Epoch: [0]  [120/159]  eta: 0:07:28  lr: 0.000029  loss: 1.4194 (1.5620)  time: 27.2529  data: 26.7262  max mem: 29171\n",
            "Epoch: [0]  [130/159]  eta: 0:06:06  lr: 0.000029  loss: 1.4290 (1.5511)  time: 26.8140  data: 26.2874  max mem: 29171\n",
            "Epoch: [0]  [140/159]  eta: 0:04:20  lr: 0.000029  loss: 1.4192 (1.5438)  time: 27.0795  data: 26.5529  max mem: 29171\n",
            "Epoch: [0]  [150/159]  eta: 0:02:11  lr: 0.000029  loss: 1.4035 (1.5329)  time: 27.2137  data: 26.6882  max mem: 29171\n",
            "Epoch: [0]  [158/159]  eta: 0:00:15  lr: 0.000029  loss: 1.3794 (1.5223)  time: 26.6761  data: 26.1504  max mem: 29171\n",
            "Epoch: [0] Total time: 0:40:14 (15.1829 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 1.3794 (1.5223)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/ViT_Training/engine.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:  [ 0/27]  eta: 0:47:46  loss: 0.2202 (0.2202)  acc1: 95.1111 (95.1111)  acc5: 99.1111 (99.1111)  time: 106.1689  data: 105.6554  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:30:05  loss: 0.5534 (0.5931)  acc1: 86.4444 (84.9293)  acc5: 96.4445 (96.0404)  time: 106.1859  data: 105.8617  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:12:22  loss: 0.7517 (0.6943)  acc1: 79.7778 (82.3069)  acc5: 95.1111 (95.0159)  time: 106.0648  data: 105.7605  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:01:45  loss: 0.7594 (0.6851)  acc1: 79.7778 (82.4333)  acc5: 94.0000 (95.2917)  time: 104.5265  data: 104.2273  max mem: 29171\n",
            "Test: Total time: 0:47:17 (105.0823 s / it)\n",
            "* Acc@1 82.433 Acc@5 95.292 loss 0.685\n",
            "Accuracy of the network on the 12000 test images: 82.4%\n",
            "Max accuracy: 82.43%\n",
            "Epoch: [1]  [  0/159]  eta: 0:13:26  lr: 0.000029  loss: 1.3633 (1.3633)  time: 5.0730  data: 4.5199  max mem: 29171\n",
            "Epoch: [1]  [ 10/159]  eta: 0:12:31  lr: 0.000029  loss: 1.3463 (1.3287)  time: 5.0439  data: 4.5150  max mem: 29171\n",
            "Epoch: [1]  [ 20/159]  eta: 0:11:43  lr: 0.000029  loss: 1.3917 (1.3855)  time: 5.0618  data: 4.5364  max mem: 29171\n",
            "Epoch: [1]  [ 30/159]  eta: 0:10:55  lr: 0.000029  loss: 1.4279 (1.3813)  time: 5.0970  data: 4.5715  max mem: 29171\n",
            "Epoch: [1]  [ 40/159]  eta: 0:10:04  lr: 0.000029  loss: 1.3735 (1.3657)  time: 5.0992  data: 4.5712  max mem: 29171\n",
            "Epoch: [1]  [ 50/159]  eta: 0:09:13  lr: 0.000029  loss: 1.3492 (1.3553)  time: 5.0715  data: 4.5419  max mem: 29171\n",
            "Epoch: [1]  [ 60/159]  eta: 0:08:21  lr: 0.000029  loss: 1.2930 (1.3401)  time: 5.0228  data: 4.4930  max mem: 29171\n",
            "Epoch: [1]  [ 70/159]  eta: 0:07:31  lr: 0.000029  loss: 1.3387 (1.3482)  time: 5.0615  data: 4.5325  max mem: 29171\n",
            "Epoch: [1]  [ 80/159]  eta: 0:06:40  lr: 0.000029  loss: 1.2973 (1.3339)  time: 5.0932  data: 4.5653  max mem: 29171\n",
            "Epoch: [1]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 1.2627 (1.3326)  time: 5.0083  data: 4.4799  max mem: 29171\n",
            "Epoch: [1]  [100/159]  eta: 0:04:58  lr: 0.000029  loss: 1.2627 (1.3243)  time: 5.0385  data: 4.5104  max mem: 29171\n",
            "Epoch: [1]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 1.2461 (1.3201)  time: 5.0684  data: 4.5408  max mem: 29171\n",
            "Epoch: [1]  [120/159]  eta: 0:03:17  lr: 0.000029  loss: 1.2741 (1.3191)  time: 5.0087  data: 4.4805  max mem: 29171\n",
            "Epoch: [1]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 1.2914 (1.3152)  time: 4.9855  data: 4.4582  max mem: 29171\n",
            "Epoch: [1]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 1.2956 (1.3144)  time: 5.0167  data: 4.4893  max mem: 29171\n",
            "Epoch: [1]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 1.2827 (1.3104)  time: 5.0458  data: 4.5185  max mem: 29171\n",
            "Epoch: [1]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 1.2767 (1.3040)  time: 5.0267  data: 4.4999  max mem: 29171\n",
            "Epoch: [1] Total time: 0:13:22 (5.0462 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 1.2767 (1.3040)\n",
            "Test:  [ 0/27]  eta: 0:03:23  loss: 0.1780 (0.1780)  acc1: 96.0000 (96.0000)  acc5: 99.3333 (99.3333)  time: 7.5378  data: 7.2295  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.4744 (0.5130)  acc1: 87.7778 (87.3333)  acc5: 96.8889 (96.7677)  time: 7.0929  data: 6.7891  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.6467 (0.5994)  acc1: 82.6667 (84.9312)  acc5: 96.2222 (95.9153)  time: 7.0398  data: 6.7369  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.6468 (0.5884)  acc1: 83.3333 (85.1250)  acc5: 96.0000 (96.1750)  time: 6.9818  data: 6.6834  max mem: 29171\n",
            "Test: Total time: 0:03:09 (7.0118 s / it)\n",
            "* Acc@1 85.125 Acc@5 96.175 loss 0.588\n",
            "Accuracy of the network on the 12000 test images: 85.1%\n",
            "Max accuracy: 85.13%\n",
            "Epoch: [2]  [  0/159]  eta: 0:13:27  lr: 0.000029  loss: 1.3355 (1.3355)  time: 5.0813  data: 4.5519  max mem: 29171\n",
            "Epoch: [2]  [ 10/159]  eta: 0:12:35  lr: 0.000029  loss: 1.2047 (1.2033)  time: 5.0687  data: 4.5394  max mem: 29171\n",
            "Epoch: [2]  [ 20/159]  eta: 0:11:47  lr: 0.000029  loss: 1.1858 (1.2337)  time: 5.0905  data: 4.5617  max mem: 29171\n",
            "Epoch: [2]  [ 30/159]  eta: 0:10:54  lr: 0.000029  loss: 1.2078 (1.2323)  time: 5.0787  data: 4.5513  max mem: 29171\n",
            "Epoch: [2]  [ 40/159]  eta: 0:10:01  lr: 0.000029  loss: 1.2617 (1.2267)  time: 5.0100  data: 4.4836  max mem: 29171\n",
            "Epoch: [2]  [ 50/159]  eta: 0:09:07  lr: 0.000029  loss: 1.1985 (1.2145)  time: 4.9386  data: 4.4113  max mem: 29171\n",
            "Epoch: [2]  [ 60/159]  eta: 0:08:16  lr: 0.000029  loss: 1.1320 (1.1973)  time: 4.9448  data: 4.4178  max mem: 29171\n",
            "Epoch: [2]  [ 70/159]  eta: 0:07:28  lr: 0.000029  loss: 1.1312 (1.1982)  time: 5.0751  data: 4.5497  max mem: 29171\n",
            "Epoch: [2]  [ 80/159]  eta: 0:06:37  lr: 0.000029  loss: 1.1437 (1.1892)  time: 5.0836  data: 4.5583  max mem: 29171\n",
            "Epoch: [2]  [ 90/159]  eta: 0:05:46  lr: 0.000029  loss: 1.1476 (1.1908)  time: 4.9661  data: 4.4399  max mem: 29171\n",
            "Epoch: [2]  [100/159]  eta: 0:04:56  lr: 0.000029  loss: 1.2106 (1.1894)  time: 5.0057  data: 4.4757  max mem: 29171\n",
            "Epoch: [2]  [110/159]  eta: 0:04:06  lr: 0.000029  loss: 1.1694 (1.1805)  time: 5.0607  data: 4.5299  max mem: 29171\n",
            "Epoch: [2]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 1.0965 (1.1780)  time: 5.0273  data: 4.4994  max mem: 29171\n",
            "Epoch: [2]  [130/159]  eta: 0:02:25  lr: 0.000029  loss: 1.1228 (1.1794)  time: 4.9729  data: 4.4451  max mem: 29171\n",
            "Epoch: [2]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 1.1436 (1.1825)  time: 5.0154  data: 4.4868  max mem: 29171\n",
            "Epoch: [2]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 1.1436 (1.1813)  time: 5.0995  data: 4.5717  max mem: 29171\n",
            "Epoch: [2]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 1.1633 (1.1788)  time: 5.0669  data: 4.5397  max mem: 29171\n",
            "Epoch: [2] Total time: 0:13:19 (5.0285 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 1.1633 (1.1788)\n",
            "Test:  [ 0/27]  eta: 0:03:22  loss: 0.1546 (0.1546)  acc1: 96.2222 (96.2222)  acc5: 99.3333 (99.3333)  time: 7.5162  data: 7.2144  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.4203 (0.4606)  acc1: 88.6667 (88.9697)  acc5: 97.3333 (97.1111)  time: 7.0979  data: 6.7943  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.5743 (0.5370)  acc1: 85.5556 (86.7196)  acc5: 96.4445 (96.3915)  time: 7.0320  data: 6.7282  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:06  loss: 0.5766 (0.5263)  acc1: 84.0000 (86.8250)  acc5: 95.7778 (96.6333)  time: 6.9701  data: 6.6703  max mem: 29171\n",
            "Test: Total time: 0:03:09 (7.0001 s / it)\n",
            "* Acc@1 86.825 Acc@5 96.633 loss 0.526\n",
            "Accuracy of the network on the 12000 test images: 86.8%\n",
            "Max accuracy: 86.83%\n",
            "Epoch: [3]  [  0/159]  eta: 0:13:55  lr: 0.000029  loss: 1.2436 (1.2436)  time: 5.2528  data: 4.7225  max mem: 29171\n",
            "Epoch: [3]  [ 10/159]  eta: 0:12:36  lr: 0.000029  loss: 1.1145 (1.1009)  time: 5.0744  data: 4.5441  max mem: 29171\n",
            "Epoch: [3]  [ 20/159]  eta: 0:11:42  lr: 0.000029  loss: 1.0912 (1.1317)  time: 5.0438  data: 4.5153  max mem: 29171\n",
            "Epoch: [3]  [ 30/159]  eta: 0:10:51  lr: 0.000029  loss: 1.1366 (1.1324)  time: 5.0430  data: 4.5159  max mem: 29171\n",
            "Epoch: [3]  [ 40/159]  eta: 0:09:59  lr: 0.000029  loss: 1.1044 (1.1214)  time: 5.0230  data: 4.4958  max mem: 29171\n",
            "Epoch: [3]  [ 50/159]  eta: 0:09:07  lr: 0.000029  loss: 1.0513 (1.0998)  time: 4.9851  data: 4.4500  max mem: 29171\n",
            "Epoch: [3]  [ 60/159]  eta: 0:08:17  lr: 0.000029  loss: 1.0319 (1.0885)  time: 5.0077  data: 4.4722  max mem: 29171\n",
            "Epoch: [3]  [ 70/159]  eta: 0:07:30  lr: 0.000029  loss: 1.0470 (1.0911)  time: 5.1354  data: 4.6072  max mem: 29171\n",
            "Epoch: [3]  [ 80/159]  eta: 0:06:40  lr: 0.000029  loss: 1.0523 (1.0868)  time: 5.1763  data: 4.6478  max mem: 29171\n",
            "Epoch: [3]  [ 90/159]  eta: 0:05:49  lr: 0.000029  loss: 1.0756 (1.0887)  time: 5.0981  data: 4.5697  max mem: 29171\n",
            "Epoch: [3]  [100/159]  eta: 0:04:59  lr: 0.000029  loss: 1.0713 (1.0857)  time: 5.1257  data: 4.5962  max mem: 29171\n",
            "Epoch: [3]  [110/159]  eta: 0:04:08  lr: 0.000029  loss: 1.0305 (1.0797)  time: 5.1267  data: 4.5973  max mem: 29171\n",
            "Epoch: [3]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 1.0222 (1.0785)  time: 5.0898  data: 4.5628  max mem: 29171\n",
            "Epoch: [3]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 1.0166 (1.0769)  time: 5.0719  data: 4.5457  max mem: 29171\n",
            "Epoch: [3]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 1.0886 (1.0813)  time: 5.0668  data: 4.5399  max mem: 29171\n",
            "Epoch: [3]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 1.0930 (1.0816)  time: 5.1026  data: 4.5751  max mem: 29171\n",
            "Epoch: [3]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 1.0767 (1.0802)  time: 5.1145  data: 4.5869  max mem: 29171\n",
            "Epoch: [3] Total time: 0:13:28 (5.0825 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 1.0767 (1.0802)\n",
            "Test:  [ 0/27]  eta: 0:03:27  loss: 0.1410 (0.1410)  acc1: 96.6667 (96.6667)  acc5: 99.3333 (99.3333)  time: 7.6830  data: 7.3794  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:02  loss: 0.3779 (0.4183)  acc1: 90.6667 (90.1010)  acc5: 97.3333 (97.3131)  time: 7.2286  data: 6.9227  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.5153 (0.4884)  acc1: 87.1111 (88.1058)  acc5: 96.8889 (96.7090)  time: 7.1556  data: 6.8505  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.5250 (0.4782)  acc1: 85.7778 (88.1667)  acc5: 96.8889 (96.9833)  time: 7.0768  data: 6.7766  max mem: 29171\n",
            "Test: Total time: 0:03:12 (7.1257 s / it)\n",
            "* Acc@1 88.167 Acc@5 96.983 loss 0.478\n",
            "Accuracy of the network on the 12000 test images: 88.2%\n",
            "Max accuracy: 88.17%\n",
            "Epoch: [4]  [  0/159]  eta: 0:14:09  lr: 0.000029  loss: 1.1565 (1.1565)  time: 5.3417  data: 4.8172  max mem: 29171\n",
            "Epoch: [4]  [ 10/159]  eta: 0:12:52  lr: 0.000029  loss: 1.0151 (1.0134)  time: 5.1814  data: 4.6532  max mem: 29171\n",
            "Epoch: [4]  [ 20/159]  eta: 0:11:58  lr: 0.000029  loss: 1.0151 (1.0464)  time: 5.1623  data: 4.6338  max mem: 29171\n",
            "Epoch: [4]  [ 30/159]  eta: 0:11:04  lr: 0.000029  loss: 1.0292 (1.0504)  time: 5.1378  data: 4.6093  max mem: 29171\n",
            "Epoch: [4]  [ 40/159]  eta: 0:10:10  lr: 0.000029  loss: 1.0472 (1.0423)  time: 5.0863  data: 4.5575  max mem: 29171\n",
            "Epoch: [4]  [ 50/159]  eta: 0:09:15  lr: 0.000029  loss: 1.0237 (1.0332)  time: 5.0094  data: 4.4804  max mem: 29171\n",
            "Epoch: [4]  [ 60/159]  eta: 0:08:24  lr: 0.000029  loss: 1.0237 (1.0319)  time: 5.0287  data: 4.5008  max mem: 29171\n",
            "Epoch: [4]  [ 70/159]  eta: 0:07:34  lr: 0.000029  loss: 1.0531 (1.0360)  time: 5.1462  data: 4.6192  max mem: 29171\n",
            "Epoch: [4]  [ 80/159]  eta: 0:06:43  lr: 0.000029  loss: 1.0493 (1.0328)  time: 5.1605  data: 4.6327  max mem: 29171\n",
            "Epoch: [4]  [ 90/159]  eta: 0:05:51  lr: 0.000029  loss: 1.0216 (1.0363)  time: 5.0583  data: 4.5299  max mem: 29171\n",
            "Epoch: [4]  [100/159]  eta: 0:05:01  lr: 0.000029  loss: 1.0216 (1.0341)  time: 5.0700  data: 4.5411  max mem: 29171\n",
            "Epoch: [4]  [110/159]  eta: 0:04:09  lr: 0.000029  loss: 0.9795 (1.0330)  time: 5.0966  data: 4.5696  max mem: 29171\n",
            "Epoch: [4]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 0.9389 (1.0281)  time: 5.0666  data: 4.5409  max mem: 29171\n",
            "Epoch: [4]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 0.9665 (1.0298)  time: 5.0734  data: 4.5466  max mem: 29171\n",
            "Epoch: [4]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.9725 (1.0266)  time: 5.0576  data: 4.5310  max mem: 29171\n",
            "Epoch: [4]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 1.0039 (1.0265)  time: 5.0675  data: 4.5409  max mem: 29171\n",
            "Epoch: [4]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.9524 (1.0236)  time: 5.0702  data: 4.5441  max mem: 29171\n",
            "Epoch: [4] Total time: 0:13:29 (5.0891 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.9524 (1.0236)\n",
            "Test:  [ 0/27]  eta: 0:03:22  loss: 0.1189 (0.1189)  acc1: 97.3333 (97.3333)  acc5: 99.3333 (99.3333)  time: 7.4948  data: 7.1938  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.3453 (0.3829)  acc1: 91.1111 (91.0909)  acc5: 97.5556 (97.4747)  time: 7.1379  data: 6.8338  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.4756 (0.4505)  acc1: 87.3333 (89.0899)  acc5: 96.8889 (96.8889)  time: 7.1278  data: 6.8226  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.4922 (0.4405)  acc1: 87.1111 (89.1250)  acc5: 96.6667 (97.1500)  time: 7.1251  data: 6.8238  max mem: 29171\n",
            "Test: Total time: 0:03:12 (7.1301 s / it)\n",
            "* Acc@1 89.125 Acc@5 97.150 loss 0.441\n",
            "Accuracy of the network on the 12000 test images: 89.1%\n",
            "Max accuracy: 89.13%\n",
            "Epoch: [5]  [  0/159]  eta: 0:14:05  lr: 0.000029  loss: 0.9329 (0.9329)  time: 5.3170  data: 4.7900  max mem: 29171\n",
            "Epoch: [5]  [ 10/159]  eta: 0:12:52  lr: 0.000029  loss: 0.9329 (0.9534)  time: 5.1859  data: 4.6575  max mem: 29171\n",
            "Epoch: [5]  [ 20/159]  eta: 0:12:09  lr: 0.000029  loss: 0.9713 (0.9890)  time: 5.2460  data: 4.7176  max mem: 29171\n",
            "Epoch: [5]  [ 30/159]  eta: 0:11:11  lr: 0.000029  loss: 0.9713 (0.9883)  time: 5.2104  data: 4.6812  max mem: 29171\n",
            "Epoch: [5]  [ 40/159]  eta: 0:10:14  lr: 0.000029  loss: 0.9307 (0.9844)  time: 5.0777  data: 4.5489  max mem: 29171\n",
            "Epoch: [5]  [ 50/159]  eta: 0:09:18  lr: 0.000029  loss: 0.9307 (0.9679)  time: 5.0033  data: 4.4762  max mem: 29171\n",
            "Epoch: [5]  [ 60/159]  eta: 0:08:25  lr: 0.000029  loss: 0.9059 (0.9645)  time: 4.9992  data: 4.4717  max mem: 29171\n",
            "Epoch: [5]  [ 70/159]  eta: 0:07:35  lr: 0.000029  loss: 0.9847 (0.9679)  time: 5.0832  data: 4.5554  max mem: 29171\n",
            "Epoch: [5]  [ 80/159]  eta: 0:06:42  lr: 0.000029  loss: 0.9490 (0.9622)  time: 5.0602  data: 4.5334  max mem: 29171\n",
            "Epoch: [5]  [ 90/159]  eta: 0:05:50  lr: 0.000029  loss: 0.9387 (0.9622)  time: 4.9768  data: 4.4512  max mem: 29171\n",
            "Epoch: [5]  [100/159]  eta: 0:05:00  lr: 0.000029  loss: 0.9575 (0.9597)  time: 5.0697  data: 4.5438  max mem: 29171\n",
            "Epoch: [5]  [110/159]  eta: 0:04:09  lr: 0.000029  loss: 0.9575 (0.9590)  time: 5.1279  data: 4.6009  max mem: 29171\n",
            "Epoch: [5]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 0.9332 (0.9591)  time: 5.0707  data: 4.5433  max mem: 29171\n",
            "Epoch: [5]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 0.9332 (0.9548)  time: 5.0784  data: 4.5521  max mem: 29171\n",
            "Epoch: [5]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.9492 (0.9572)  time: 5.0804  data: 4.5540  max mem: 29171\n",
            "Epoch: [5]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.9492 (0.9545)  time: 5.0675  data: 4.5400  max mem: 29171\n",
            "Epoch: [5]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.9346 (0.9518)  time: 5.0670  data: 4.5400  max mem: 29171\n",
            "Epoch: [5] Total time: 0:13:28 (5.0849 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.9346 (0.9518)\n",
            "Test:  [ 0/27]  eta: 0:03:26  loss: 0.1048 (0.1048)  acc1: 98.0000 (98.0000)  acc5: 99.3333 (99.3333)  time: 7.6472  data: 7.3380  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:02  loss: 0.3267 (0.3563)  acc1: 91.1111 (91.5556)  acc5: 97.7778 (97.6566)  time: 7.2273  data: 6.9223  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.4368 (0.4216)  acc1: 87.3333 (89.5344)  acc5: 97.3333 (97.2381)  time: 7.1595  data: 6.8549  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.4633 (0.4117)  acc1: 87.1111 (89.6083)  acc5: 97.1111 (97.4667)  time: 7.0830  data: 6.7826  max mem: 29171\n",
            "Test: Total time: 0:03:12 (7.1165 s / it)\n",
            "* Acc@1 89.608 Acc@5 97.467 loss 0.412\n",
            "Accuracy of the network on the 12000 test images: 89.6%\n",
            "Max accuracy: 89.61%\n",
            "Epoch: [6]  [  0/159]  eta: 0:14:26  lr: 0.000029  loss: 0.9509 (0.9509)  time: 5.4510  data: 4.9242  max mem: 29171\n",
            "Epoch: [6]  [ 10/159]  eta: 0:12:51  lr: 0.000029  loss: 0.9509 (0.9494)  time: 5.1802  data: 4.6519  max mem: 29171\n",
            "Epoch: [6]  [ 20/159]  eta: 0:11:53  lr: 0.000029  loss: 0.9838 (0.9765)  time: 5.1144  data: 4.5872  max mem: 29171\n",
            "Epoch: [6]  [ 30/159]  eta: 0:10:59  lr: 0.000029  loss: 0.9838 (0.9672)  time: 5.0708  data: 4.5436  max mem: 29171\n",
            "Epoch: [6]  [ 40/159]  eta: 0:10:06  lr: 0.000029  loss: 0.8977 (0.9525)  time: 5.0569  data: 4.5289  max mem: 29171\n",
            "Epoch: [6]  [ 50/159]  eta: 0:09:12  lr: 0.000029  loss: 0.8708 (0.9372)  time: 5.0143  data: 4.4874  max mem: 29171\n",
            "Epoch: [6]  [ 60/159]  eta: 0:08:21  lr: 0.000029  loss: 0.8980 (0.9357)  time: 5.0195  data: 4.4923  max mem: 29171\n",
            "Epoch: [6]  [ 70/159]  eta: 0:07:32  lr: 0.000029  loss: 0.9564 (0.9406)  time: 5.1252  data: 4.5973  max mem: 29171\n",
            "Epoch: [6]  [ 80/159]  eta: 0:06:41  lr: 0.000029  loss: 0.9005 (0.9309)  time: 5.1126  data: 4.5848  max mem: 29171\n",
            "Epoch: [6]  [ 90/159]  eta: 0:05:49  lr: 0.000029  loss: 0.8827 (0.9310)  time: 5.0172  data: 4.4892  max mem: 29171\n",
            "Epoch: [6]  [100/159]  eta: 0:04:59  lr: 0.000029  loss: 0.8686 (0.9191)  time: 5.0913  data: 4.5628  max mem: 29171\n",
            "Epoch: [6]  [110/159]  eta: 0:04:09  lr: 0.000029  loss: 0.8130 (0.9143)  time: 5.1457  data: 4.6093  max mem: 29171\n",
            "Epoch: [6]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 0.9062 (0.9130)  time: 5.0790  data: 4.5441  max mem: 29171\n",
            "Epoch: [6]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 0.8666 (0.9134)  time: 5.0487  data: 4.5230  max mem: 29171\n",
            "Epoch: [6]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.8595 (0.9120)  time: 5.0667  data: 4.5409  max mem: 29171\n",
            "Epoch: [6]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.8669 (0.9113)  time: 5.0992  data: 4.5721  max mem: 29171\n",
            "Epoch: [6]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.8393 (0.9094)  time: 5.1135  data: 4.5849  max mem: 29171\n",
            "Epoch: [6] Total time: 0:13:28 (5.0839 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.8393 (0.9094)\n",
            "Test:  [ 0/27]  eta: 0:03:27  loss: 0.1032 (0.1032)  acc1: 97.5556 (97.5556)  acc5: 99.3333 (99.3333)  time: 7.7028  data: 7.3941  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.3143 (0.3342)  acc1: 91.3333 (91.8990)  acc5: 97.7778 (97.8182)  time: 7.1328  data: 6.8257  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.4077 (0.3974)  acc1: 88.8889 (90.1587)  acc5: 97.3333 (97.3968)  time: 7.0772  data: 6.7713  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.4492 (0.3888)  acc1: 88.2222 (90.1083)  acc5: 97.5556 (97.6333)  time: 7.0447  data: 6.7443  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0594 s / it)\n",
            "* Acc@1 90.108 Acc@5 97.633 loss 0.389\n",
            "Accuracy of the network on the 12000 test images: 90.1%\n",
            "Max accuracy: 90.11%\n",
            "Epoch: [7]  [  0/159]  eta: 0:13:52  lr: 0.000029  loss: 0.8836 (0.8836)  time: 5.2370  data: 4.7119  max mem: 29171\n",
            "Epoch: [7]  [ 10/159]  eta: 0:12:45  lr: 0.000029  loss: 0.7819 (0.8131)  time: 5.1376  data: 4.6103  max mem: 29171\n",
            "Epoch: [7]  [ 20/159]  eta: 0:11:49  lr: 0.000029  loss: 0.8825 (0.8853)  time: 5.1011  data: 4.5728  max mem: 29171\n",
            "Epoch: [7]  [ 30/159]  eta: 0:10:57  lr: 0.000029  loss: 0.9219 (0.8838)  time: 5.0689  data: 4.5416  max mem: 29171\n",
            "Epoch: [7]  [ 40/159]  eta: 0:10:04  lr: 0.000029  loss: 0.8589 (0.8845)  time: 5.0508  data: 4.5247  max mem: 29171\n",
            "Epoch: [7]  [ 50/159]  eta: 0:09:10  lr: 0.000029  loss: 0.8104 (0.8746)  time: 4.9926  data: 4.4661  max mem: 29171\n",
            "Epoch: [7]  [ 60/159]  eta: 0:08:19  lr: 0.000029  loss: 0.8036 (0.8703)  time: 4.9845  data: 4.4586  max mem: 29171\n",
            "Epoch: [7]  [ 70/159]  eta: 0:07:30  lr: 0.000029  loss: 0.8036 (0.8689)  time: 5.0751  data: 4.5495  max mem: 29171\n",
            "Epoch: [7]  [ 80/159]  eta: 0:06:39  lr: 0.000029  loss: 0.8242 (0.8688)  time: 5.0973  data: 4.5699  max mem: 29171\n",
            "Epoch: [7]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 0.8854 (0.8734)  time: 5.0397  data: 4.5105  max mem: 29171\n",
            "Epoch: [7]  [100/159]  eta: 0:04:58  lr: 0.000029  loss: 0.9066 (0.8759)  time: 5.0616  data: 4.5337  max mem: 29171\n",
            "Epoch: [7]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.8909 (0.8746)  time: 5.0820  data: 4.5564  max mem: 29171\n",
            "Epoch: [7]  [120/159]  eta: 0:03:17  lr: 0.000029  loss: 0.7985 (0.8717)  time: 5.0473  data: 4.5213  max mem: 29171\n",
            "Epoch: [7]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.8286 (0.8693)  time: 5.0231  data: 4.4965  max mem: 29171\n",
            "Epoch: [7]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.8378 (0.8712)  time: 5.0510  data: 4.5171  max mem: 29171\n",
            "Epoch: [7]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.8787 (0.8713)  time: 5.0814  data: 4.5477  max mem: 29171\n",
            "Epoch: [7]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.8490 (0.8698)  time: 5.0455  data: 4.5199  max mem: 29171\n",
            "Epoch: [7] Total time: 0:13:23 (5.0549 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.8490 (0.8698)\n",
            "Test:  [ 0/27]  eta: 0:03:26  loss: 0.0967 (0.0967)  acc1: 97.5556 (97.5556)  acc5: 99.5556 (99.5556)  time: 7.6344  data: 7.3326  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.3036 (0.3203)  acc1: 92.2222 (92.3636)  acc5: 98.0000 (98.0404)  time: 7.1214  data: 6.8180  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.3891 (0.3770)  acc1: 90.0000 (90.6138)  acc5: 97.5556 (97.5873)  time: 7.1237  data: 6.8202  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.4228 (0.3690)  acc1: 88.0000 (90.6083)  acc5: 97.3333 (97.7917)  time: 7.2217  data: 6.9220  max mem: 29171\n",
            "Test: Total time: 0:03:14 (7.1887 s / it)\n",
            "* Acc@1 90.608 Acc@5 97.792 loss 0.369\n",
            "Accuracy of the network on the 12000 test images: 90.6%\n",
            "Max accuracy: 90.61%\n",
            "Epoch: [8]  [  0/159]  eta: 0:14:52  lr: 0.000029  loss: 0.8169 (0.8169)  time: 5.6127  data: 5.0887  max mem: 29171\n",
            "Epoch: [8]  [ 10/159]  eta: 0:12:52  lr: 0.000029  loss: 0.8169 (0.8422)  time: 5.1865  data: 4.6598  max mem: 29171\n",
            "Epoch: [8]  [ 20/159]  eta: 0:12:00  lr: 0.000029  loss: 0.8990 (0.8918)  time: 5.1611  data: 4.6338  max mem: 29171\n",
            "Epoch: [8]  [ 30/159]  eta: 0:11:11  lr: 0.000029  loss: 0.9049 (0.8855)  time: 5.2112  data: 4.6836  max mem: 29171\n",
            "Epoch: [8]  [ 40/159]  eta: 0:10:15  lr: 0.000029  loss: 0.8677 (0.8738)  time: 5.1632  data: 4.6355  max mem: 29171\n",
            "Epoch: [8]  [ 50/159]  eta: 0:09:21  lr: 0.000029  loss: 0.7915 (0.8557)  time: 5.0607  data: 4.5331  max mem: 29171\n",
            "Epoch: [8]  [ 60/159]  eta: 0:08:30  lr: 0.000029  loss: 0.7814 (0.8445)  time: 5.1150  data: 4.5872  max mem: 29171\n",
            "Epoch: [8]  [ 70/159]  eta: 0:07:38  lr: 0.000029  loss: 0.8421 (0.8528)  time: 5.1767  data: 4.6485  max mem: 29171\n",
            "Epoch: [8]  [ 80/159]  eta: 0:06:46  lr: 0.000029  loss: 0.8634 (0.8474)  time: 5.1137  data: 4.5857  max mem: 29171\n",
            "Epoch: [8]  [ 90/159]  eta: 0:05:53  lr: 0.000029  loss: 0.7938 (0.8430)  time: 5.0017  data: 4.4737  max mem: 29171\n",
            "Epoch: [8]  [100/159]  eta: 0:05:02  lr: 0.000029  loss: 0.8171 (0.8453)  time: 5.0512  data: 4.5234  max mem: 29171\n",
            "Epoch: [8]  [110/159]  eta: 0:04:11  lr: 0.000029  loss: 0.8346 (0.8439)  time: 5.1455  data: 4.6170  max mem: 29171\n",
            "Epoch: [8]  [120/159]  eta: 0:03:19  lr: 0.000029  loss: 0.7891 (0.8402)  time: 5.1002  data: 4.5728  max mem: 29171\n",
            "Epoch: [8]  [130/159]  eta: 0:02:28  lr: 0.000029  loss: 0.8031 (0.8392)  time: 5.0759  data: 4.5482  max mem: 29171\n",
            "Epoch: [8]  [140/159]  eta: 0:01:37  lr: 0.000029  loss: 0.8674 (0.8431)  time: 5.0739  data: 4.5450  max mem: 29171\n",
            "Epoch: [8]  [150/159]  eta: 0:00:46  lr: 0.000029  loss: 0.8674 (0.8427)  time: 5.0754  data: 4.5469  max mem: 29171\n",
            "Epoch: [8]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.7916 (0.8407)  time: 5.0959  data: 4.5678  max mem: 29171\n",
            "Epoch: [8] Total time: 0:13:32 (5.1121 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.7916 (0.8407)\n",
            "Test:  [ 0/27]  eta: 0:03:22  loss: 0.0922 (0.0922)  acc1: 97.7778 (97.7778)  acc5: 99.5556 (99.5556)  time: 7.5002  data: 7.1979  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.2854 (0.3030)  acc1: 92.4445 (92.6869)  acc5: 98.0000 (98.1616)  time: 7.1531  data: 6.8484  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.3694 (0.3596)  acc1: 90.0000 (90.8783)  acc5: 97.3333 (97.7566)  time: 7.0842  data: 6.7793  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.4013 (0.3519)  acc1: 88.6667 (90.9083)  acc5: 97.3333 (97.9500)  time: 7.0095  data: 6.7097  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0425 s / it)\n",
            "* Acc@1 90.908 Acc@5 97.950 loss 0.352\n",
            "Accuracy of the network on the 12000 test images: 90.9%\n",
            "Max accuracy: 90.91%\n",
            "Epoch: [9]  [  0/159]  eta: 0:13:28  lr: 0.000029  loss: 0.9004 (0.9004)  time: 5.0844  data: 4.5553  max mem: 29171\n",
            "Epoch: [9]  [ 10/159]  eta: 0:12:43  lr: 0.000029  loss: 0.7774 (0.7955)  time: 5.1214  data: 4.5915  max mem: 29171\n",
            "Epoch: [9]  [ 20/159]  eta: 0:11:49  lr: 0.000029  loss: 0.7914 (0.8377)  time: 5.1068  data: 4.5792  max mem: 29171\n",
            "Epoch: [9]  [ 30/159]  eta: 0:10:58  lr: 0.000029  loss: 0.8743 (0.8554)  time: 5.0902  data: 4.5636  max mem: 29171\n",
            "Epoch: [9]  [ 40/159]  eta: 0:10:07  lr: 0.000029  loss: 0.8334 (0.8502)  time: 5.1051  data: 4.5764  max mem: 29171\n",
            "Epoch: [9]  [ 50/159]  eta: 0:09:13  lr: 0.000029  loss: 0.8109 (0.8350)  time: 5.0435  data: 4.5151  max mem: 29171\n",
            "Epoch: [9]  [ 60/159]  eta: 0:08:21  lr: 0.000029  loss: 0.7804 (0.8231)  time: 4.9878  data: 4.4602  max mem: 29171\n",
            "Epoch: [9]  [ 70/159]  eta: 0:07:31  lr: 0.000029  loss: 0.7940 (0.8278)  time: 5.0645  data: 4.5375  max mem: 29171\n",
            "Epoch: [9]  [ 80/159]  eta: 0:06:40  lr: 0.000029  loss: 0.7889 (0.8142)  time: 5.0609  data: 4.5345  max mem: 29171\n",
            "Epoch: [9]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 0.7691 (0.8196)  time: 4.9748  data: 4.4479  max mem: 29171\n",
            "Epoch: [9]  [100/159]  eta: 0:04:58  lr: 0.000029  loss: 0.8036 (0.8153)  time: 5.0218  data: 4.4942  max mem: 29171\n",
            "Epoch: [9]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.7464 (0.8116)  time: 5.0788  data: 4.5516  max mem: 29171\n",
            "Epoch: [9]  [120/159]  eta: 0:03:17  lr: 0.000029  loss: 0.7624 (0.8138)  time: 5.0410  data: 4.5137  max mem: 29171\n",
            "Epoch: [9]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.8034 (0.8153)  time: 4.9957  data: 4.4687  max mem: 29171\n",
            "Epoch: [9]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.7879 (0.8129)  time: 5.0071  data: 4.4800  max mem: 29171\n",
            "Epoch: [9]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.7369 (0.8130)  time: 5.0515  data: 4.5240  max mem: 29171\n",
            "Epoch: [9]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.7141 (0.8119)  time: 5.0547  data: 4.5281  max mem: 29171\n",
            "Epoch: [9] Total time: 0:13:22 (5.0480 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.7141 (0.8119)\n",
            "Test:  [ 0/27]  eta: 0:03:26  loss: 0.0888 (0.0888)  acc1: 97.5556 (97.5556)  acc5: 99.5556 (99.5556)  time: 7.6514  data: 7.3506  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.2737 (0.2914)  acc1: 92.8889 (93.0707)  acc5: 98.0000 (98.2626)  time: 7.1275  data: 6.8244  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.3539 (0.3464)  acc1: 90.6667 (91.2593)  acc5: 97.7778 (97.8624)  time: 7.0562  data: 6.7530  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3940 (0.3377)  acc1: 89.3333 (91.3417)  acc5: 97.7778 (98.0583)  time: 6.9962  data: 6.6970  max mem: 29171\n",
            "Test: Total time: 0:03:09 (7.0295 s / it)\n",
            "* Acc@1 91.342 Acc@5 98.058 loss 0.338\n",
            "Accuracy of the network on the 12000 test images: 91.3%\n",
            "Max accuracy: 91.34%\n",
            "Epoch: [10]  [  0/159]  eta: 0:13:56  lr: 0.000029  loss: 0.7341 (0.7341)  time: 5.2593  data: 4.7325  max mem: 29171\n",
            "Epoch: [10]  [ 10/159]  eta: 0:12:45  lr: 0.000029  loss: 0.7340 (0.7674)  time: 5.1376  data: 4.6080  max mem: 29171\n",
            "Epoch: [10]  [ 20/159]  eta: 0:11:53  lr: 0.000029  loss: 0.7885 (0.7874)  time: 5.1231  data: 4.5943  max mem: 29171\n",
            "Epoch: [10]  [ 30/159]  eta: 0:10:58  lr: 0.000029  loss: 0.7885 (0.7900)  time: 5.0815  data: 4.5535  max mem: 29171\n",
            "Epoch: [10]  [ 40/159]  eta: 0:10:04  lr: 0.000029  loss: 0.7742 (0.7894)  time: 5.0289  data: 4.5007  max mem: 29171\n",
            "Epoch: [10]  [ 50/159]  eta: 0:09:10  lr: 0.000029  loss: 0.7315 (0.7793)  time: 4.9749  data: 4.4470  max mem: 29171\n",
            "Epoch: [10]  [ 60/159]  eta: 0:08:19  lr: 0.000029  loss: 0.7248 (0.7719)  time: 4.9751  data: 4.4474  max mem: 29171\n",
            "Epoch: [10]  [ 70/159]  eta: 0:07:31  lr: 0.000029  loss: 0.7343 (0.7808)  time: 5.1383  data: 4.6094  max mem: 29171\n",
            "Epoch: [10]  [ 80/159]  eta: 0:06:41  lr: 0.000029  loss: 0.7331 (0.7750)  time: 5.1849  data: 4.6563  max mem: 29171\n",
            "Epoch: [10]  [ 90/159]  eta: 0:05:51  lr: 0.000029  loss: 0.7200 (0.7713)  time: 5.1330  data: 4.6041  max mem: 29171\n",
            "Epoch: [10]  [100/159]  eta: 0:05:01  lr: 0.000029  loss: 0.7283 (0.7702)  time: 5.1942  data: 4.6648  max mem: 29171\n",
            "Epoch: [10]  [110/159]  eta: 0:04:10  lr: 0.000029  loss: 0.7283 (0.7669)  time: 5.1823  data: 4.6546  max mem: 29171\n",
            "Epoch: [10]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 0.7993 (0.7678)  time: 5.0802  data: 4.5524  max mem: 29171\n",
            "Epoch: [10]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 0.8126 (0.7693)  time: 5.0260  data: 4.4984  max mem: 29171\n",
            "Epoch: [10]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.8200 (0.7707)  time: 5.0484  data: 4.5219  max mem: 29171\n",
            "Epoch: [10]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.7731 (0.7700)  time: 5.0794  data: 4.5537  max mem: 29171\n",
            "Epoch: [10]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.7504 (0.7673)  time: 5.0905  data: 4.5646  max mem: 29171\n",
            "Epoch: [10] Total time: 0:13:29 (5.0908 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.7504 (0.7673)\n",
            "Test:  [ 0/27]  eta: 0:03:28  loss: 0.0818 (0.0818)  acc1: 98.0000 (98.0000)  acc5: 99.5556 (99.5556)  time: 7.7039  data: 7.4005  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:02  loss: 0.2657 (0.2815)  acc1: 93.3333 (93.2121)  acc5: 98.4445 (98.3232)  time: 7.2021  data: 6.8978  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.3348 (0.3343)  acc1: 91.1111 (91.5132)  acc5: 97.7778 (97.9788)  time: 7.1073  data: 6.8030  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3792 (0.3260)  acc1: 89.5556 (91.5583)  acc5: 97.7778 (98.1750)  time: 7.0439  data: 6.7443  max mem: 29171\n",
            "Test: Total time: 0:03:11 (7.0861 s / it)\n",
            "* Acc@1 91.558 Acc@5 98.175 loss 0.326\n",
            "Accuracy of the network on the 12000 test images: 91.6%\n",
            "Max accuracy: 91.56%\n",
            "Epoch: [11]  [  0/159]  eta: 0:13:25  lr: 0.000029  loss: 0.7955 (0.7955)  time: 5.0631  data: 4.5381  max mem: 29171\n",
            "Epoch: [11]  [ 10/159]  eta: 0:12:50  lr: 0.000029  loss: 0.7351 (0.7573)  time: 5.1715  data: 4.6455  max mem: 29171\n",
            "Epoch: [11]  [ 20/159]  eta: 0:12:01  lr: 0.000029  loss: 0.7351 (0.7933)  time: 5.1978  data: 4.6702  max mem: 29171\n",
            "Epoch: [11]  [ 30/159]  eta: 0:11:06  lr: 0.000029  loss: 0.7356 (0.7836)  time: 5.1681  data: 4.6394  max mem: 29171\n",
            "Epoch: [11]  [ 40/159]  eta: 0:10:10  lr: 0.000029  loss: 0.7729 (0.7702)  time: 5.0646  data: 4.5371  max mem: 29171\n",
            "Epoch: [11]  [ 50/159]  eta: 0:09:14  lr: 0.000029  loss: 0.7729 (0.7587)  time: 4.9638  data: 4.4372  max mem: 29171\n",
            "Epoch: [11]  [ 60/159]  eta: 0:08:22  lr: 0.000029  loss: 0.7446 (0.7583)  time: 4.9533  data: 4.4265  max mem: 29171\n",
            "Epoch: [11]  [ 70/159]  eta: 0:07:31  lr: 0.000029  loss: 0.7446 (0.7593)  time: 5.0432  data: 4.5163  max mem: 29171\n",
            "Epoch: [11]  [ 80/159]  eta: 0:06:42  lr: 0.000029  loss: 0.7143 (0.7573)  time: 5.1548  data: 4.6266  max mem: 29171\n",
            "Epoch: [11]  [ 90/159]  eta: 0:05:50  lr: 0.000029  loss: 0.7380 (0.7581)  time: 5.1036  data: 4.5754  max mem: 29171\n",
            "Epoch: [11]  [100/159]  eta: 0:05:00  lr: 0.000029  loss: 0.7193 (0.7551)  time: 5.0674  data: 4.5404  max mem: 29171\n",
            "Epoch: [11]  [110/159]  eta: 0:04:09  lr: 0.000029  loss: 0.7125 (0.7505)  time: 5.0972  data: 4.5705  max mem: 29171\n",
            "Epoch: [11]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 0.6660 (0.7431)  time: 5.0540  data: 4.5274  max mem: 29171\n",
            "Epoch: [11]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 0.7075 (0.7440)  time: 5.0258  data: 4.4987  max mem: 29171\n",
            "Epoch: [11]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.7180 (0.7427)  time: 5.0413  data: 4.5134  max mem: 29171\n",
            "Epoch: [11]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.7180 (0.7436)  time: 5.0987  data: 4.5711  max mem: 29171\n",
            "Epoch: [11]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.7224 (0.7441)  time: 5.1015  data: 4.5749  max mem: 29171\n",
            "Epoch: [11] Total time: 0:13:27 (5.0787 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.7224 (0.7441)\n",
            "Test:  [ 0/27]  eta: 0:03:21  loss: 0.0757 (0.0757)  acc1: 98.2222 (98.2222)  acc5: 99.5556 (99.5556)  time: 7.4558  data: 7.1548  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.2758 (0.2740)  acc1: 93.7778 (93.3737)  acc5: 98.8889 (98.3838)  time: 7.1145  data: 6.8102  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.3271 (0.3268)  acc1: 91.5556 (91.5132)  acc5: 97.5556 (98.0317)  time: 7.0726  data: 6.7678  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3736 (0.3171)  acc1: 88.8889 (91.5917)  acc5: 97.5556 (98.1917)  time: 7.0212  data: 6.7214  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0450 s / it)\n",
            "* Acc@1 91.592 Acc@5 98.192 loss 0.317\n",
            "Accuracy of the network on the 12000 test images: 91.6%\n",
            "Max accuracy: 91.59%\n",
            "Epoch: [12]  [  0/159]  eta: 0:13:58  lr: 0.000029  loss: 0.7730 (0.7730)  time: 5.2724  data: 4.7522  max mem: 29171\n",
            "Epoch: [12]  [ 10/159]  eta: 0:12:53  lr: 0.000029  loss: 0.6779 (0.6942)  time: 5.1934  data: 4.6663  max mem: 29171\n",
            "Epoch: [12]  [ 20/159]  eta: 0:11:56  lr: 0.000029  loss: 0.7105 (0.7407)  time: 5.1484  data: 4.6130  max mem: 29171\n",
            "Epoch: [12]  [ 30/159]  eta: 0:11:00  lr: 0.000029  loss: 0.7509 (0.7335)  time: 5.0839  data: 4.5490  max mem: 29171\n",
            "Epoch: [12]  [ 40/159]  eta: 0:10:06  lr: 0.000029  loss: 0.7509 (0.7445)  time: 5.0404  data: 4.5134  max mem: 29171\n",
            "Epoch: [12]  [ 50/159]  eta: 0:09:12  lr: 0.000029  loss: 0.7143 (0.7324)  time: 4.9847  data: 4.4579  max mem: 29171\n",
            "Epoch: [12]  [ 60/159]  eta: 0:08:20  lr: 0.000029  loss: 0.6744 (0.7212)  time: 4.9753  data: 4.4488  max mem: 29171\n",
            "Epoch: [12]  [ 70/159]  eta: 0:07:30  lr: 0.000029  loss: 0.6744 (0.7252)  time: 5.0409  data: 4.5144  max mem: 29171\n",
            "Epoch: [12]  [ 80/159]  eta: 0:06:39  lr: 0.000029  loss: 0.6762 (0.7185)  time: 5.0461  data: 4.5198  max mem: 29171\n",
            "Epoch: [12]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 0.7089 (0.7214)  time: 4.9874  data: 4.4605  max mem: 29171\n",
            "Epoch: [12]  [100/159]  eta: 0:04:57  lr: 0.000029  loss: 0.7274 (0.7221)  time: 5.0263  data: 4.4990  max mem: 29171\n",
            "Epoch: [12]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.7274 (0.7247)  time: 5.0471  data: 4.5208  max mem: 29171\n",
            "Epoch: [12]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 0.6594 (0.7221)  time: 5.0103  data: 4.4841  max mem: 29171\n",
            "Epoch: [12]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.6594 (0.7249)  time: 4.9963  data: 4.4695  max mem: 29171\n",
            "Epoch: [12]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.7467 (0.7271)  time: 5.0283  data: 4.5009  max mem: 29171\n",
            "Epoch: [12]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6935 (0.7281)  time: 5.0724  data: 4.5449  max mem: 29171\n",
            "Epoch: [12]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6883 (0.7253)  time: 5.0829  data: 4.5558  max mem: 29171\n",
            "Epoch: [12] Total time: 0:13:22 (5.0442 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6883 (0.7253)\n",
            "Test:  [ 0/27]  eta: 0:03:27  loss: 0.0786 (0.0786)  acc1: 98.2222 (98.2222)  acc5: 99.5556 (99.5556)  time: 7.6831  data: 7.3823  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.2555 (0.2656)  acc1: 93.1111 (93.5960)  acc5: 98.8889 (98.5859)  time: 7.1487  data: 6.8433  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.3227 (0.3185)  acc1: 91.5556 (91.6720)  acc5: 98.2222 (98.2116)  time: 7.0689  data: 6.7641  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3607 (0.3099)  acc1: 89.5556 (91.7333)  acc5: 97.7778 (98.3500)  time: 7.0095  data: 6.7092  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0444 s / it)\n",
            "* Acc@1 91.733 Acc@5 98.350 loss 0.310\n",
            "Accuracy of the network on the 12000 test images: 91.7%\n",
            "Max accuracy: 91.73%\n",
            "Epoch: [13]  [  0/159]  eta: 0:13:50  lr: 0.000029  loss: 0.8332 (0.8332)  time: 5.2228  data: 4.6942  max mem: 29171\n",
            "Epoch: [13]  [ 10/159]  eta: 0:12:43  lr: 0.000029  loss: 0.6580 (0.6681)  time: 5.1209  data: 4.5938  max mem: 29171\n",
            "Epoch: [13]  [ 20/159]  eta: 0:11:49  lr: 0.000029  loss: 0.6655 (0.7224)  time: 5.0973  data: 4.5701  max mem: 29171\n",
            "Epoch: [13]  [ 30/159]  eta: 0:10:56  lr: 0.000029  loss: 0.7292 (0.7229)  time: 5.0723  data: 4.5447  max mem: 29171\n",
            "Epoch: [13]  [ 40/159]  eta: 0:10:03  lr: 0.000029  loss: 0.6868 (0.7180)  time: 5.0404  data: 4.5139  max mem: 29171\n",
            "Epoch: [13]  [ 50/159]  eta: 0:09:10  lr: 0.000029  loss: 0.6727 (0.7103)  time: 4.9933  data: 4.4677  max mem: 29171\n",
            "Epoch: [13]  [ 60/159]  eta: 0:08:20  lr: 0.000029  loss: 0.6727 (0.7080)  time: 5.0199  data: 4.4932  max mem: 29171\n",
            "Epoch: [13]  [ 70/159]  eta: 0:07:30  lr: 0.000029  loss: 0.7546 (0.7198)  time: 5.0821  data: 4.5548  max mem: 29171\n",
            "Epoch: [13]  [ 80/159]  eta: 0:06:39  lr: 0.000029  loss: 0.7280 (0.7152)  time: 5.0511  data: 4.5251  max mem: 29171\n",
            "Epoch: [13]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 0.6925 (0.7153)  time: 5.0038  data: 4.4778  max mem: 29171\n",
            "Epoch: [13]  [100/159]  eta: 0:04:58  lr: 0.000029  loss: 0.6925 (0.7160)  time: 5.0420  data: 4.5155  max mem: 29171\n",
            "Epoch: [13]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.6599 (0.7122)  time: 5.0707  data: 4.5450  max mem: 29171\n",
            "Epoch: [13]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 0.6541 (0.7094)  time: 5.0437  data: 4.5164  max mem: 29171\n",
            "Epoch: [13]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.7035 (0.7120)  time: 5.0122  data: 4.4842  max mem: 29171\n",
            "Epoch: [13]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.7167 (0.7121)  time: 5.0106  data: 4.4845  max mem: 29171\n",
            "Epoch: [13]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.7059 (0.7111)  time: 5.0343  data: 4.5088  max mem: 29171\n",
            "Epoch: [13]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.7023 (0.7081)  time: 5.0362  data: 4.5095  max mem: 29171\n",
            "Epoch: [13] Total time: 0:13:21 (5.0426 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.7023 (0.7081)\n",
            "Test:  [ 0/27]  eta: 0:03:24  loss: 0.0725 (0.0725)  acc1: 98.0000 (98.0000)  acc5: 99.5556 (99.5556)  time: 7.5760  data: 7.2698  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.2450 (0.2592)  acc1: 93.3333 (93.5758)  acc5: 99.1111 (98.6465)  time: 7.1601  data: 6.8556  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.3161 (0.3108)  acc1: 91.3333 (91.8201)  acc5: 98.2222 (98.2963)  time: 7.1103  data: 6.8049  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3478 (0.3014)  acc1: 89.3333 (91.9750)  acc5: 98.2222 (98.4167)  time: 7.0354  data: 6.7340  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0664 s / it)\n",
            "* Acc@1 91.975 Acc@5 98.417 loss 0.301\n",
            "Accuracy of the network on the 12000 test images: 92.0%\n",
            "Max accuracy: 91.98%\n",
            "Epoch: [14]  [  0/159]  eta: 0:13:56  lr: 0.000029  loss: 0.7728 (0.7728)  time: 5.2621  data: 4.7338  max mem: 29171\n",
            "Epoch: [14]  [ 10/159]  eta: 0:12:43  lr: 0.000029  loss: 0.6219 (0.6759)  time: 5.1215  data: 4.5943  max mem: 29171\n",
            "Epoch: [14]  [ 20/159]  eta: 0:11:51  lr: 0.000029  loss: 0.6475 (0.6996)  time: 5.1114  data: 4.5844  max mem: 29171\n",
            "Epoch: [14]  [ 30/159]  eta: 0:10:57  lr: 0.000029  loss: 0.6753 (0.7110)  time: 5.0839  data: 4.5571  max mem: 29171\n",
            "Epoch: [14]  [ 40/159]  eta: 0:10:04  lr: 0.000029  loss: 0.7251 (0.7146)  time: 5.0323  data: 4.5056  max mem: 29171\n",
            "Epoch: [14]  [ 50/159]  eta: 0:09:10  lr: 0.000029  loss: 0.7009 (0.7052)  time: 4.9837  data: 4.4572  max mem: 29171\n",
            "Epoch: [14]  [ 60/159]  eta: 0:08:18  lr: 0.000029  loss: 0.6778 (0.7002)  time: 4.9488  data: 4.4226  max mem: 29171\n",
            "Epoch: [14]  [ 70/159]  eta: 0:07:29  lr: 0.000029  loss: 0.7019 (0.7089)  time: 5.0278  data: 4.5008  max mem: 29171\n",
            "Epoch: [14]  [ 80/159]  eta: 0:06:38  lr: 0.000029  loss: 0.7090 (0.7043)  time: 5.0659  data: 4.5385  max mem: 29171\n",
            "Epoch: [14]  [ 90/159]  eta: 0:05:47  lr: 0.000029  loss: 0.7081 (0.7054)  time: 4.9987  data: 4.4715  max mem: 29171\n",
            "Epoch: [14]  [100/159]  eta: 0:04:57  lr: 0.000029  loss: 0.6906 (0.7039)  time: 5.0308  data: 4.4962  max mem: 29171\n",
            "Epoch: [14]  [110/159]  eta: 0:04:06  lr: 0.000029  loss: 0.6403 (0.7019)  time: 5.0582  data: 4.5233  max mem: 29171\n",
            "Epoch: [14]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 0.6220 (0.6951)  time: 5.0330  data: 4.5048  max mem: 29171\n",
            "Epoch: [14]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.6520 (0.6940)  time: 5.0217  data: 4.4949  max mem: 29171\n",
            "Epoch: [14]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.6939 (0.6958)  time: 5.0905  data: 4.5643  max mem: 29171\n",
            "Epoch: [14]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.7038 (0.6943)  time: 5.1368  data: 4.6097  max mem: 29171\n",
            "Epoch: [14]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6540 (0.6909)  time: 5.0927  data: 4.5649  max mem: 29171\n",
            "Epoch: [14] Total time: 0:13:22 (5.0489 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6540 (0.6909)\n",
            "Test:  [ 0/27]  eta: 0:03:21  loss: 0.0696 (0.0696)  acc1: 98.4445 (98.4445)  acc5: 99.5556 (99.5556)  time: 7.4677  data: 7.1671  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:02  loss: 0.2509 (0.2523)  acc1: 93.7778 (93.6970)  acc5: 98.8889 (98.6263)  time: 7.2279  data: 6.9246  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.3005 (0.3040)  acc1: 91.5556 (91.9365)  acc5: 98.2222 (98.3280)  time: 7.1544  data: 6.8507  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3382 (0.2954)  acc1: 89.5556 (91.9667)  acc5: 98.0000 (98.4667)  time: 7.0845  data: 6.7849  max mem: 29171\n",
            "Test: Total time: 0:03:12 (7.1137 s / it)\n",
            "* Acc@1 91.967 Acc@5 98.467 loss 0.295\n",
            "Accuracy of the network on the 12000 test images: 92.0%\n",
            "Max accuracy: 91.98%\n",
            "Epoch: [15]  [  0/159]  eta: 0:13:45  lr: 0.000029  loss: 0.8603 (0.8603)  time: 5.1928  data: 4.6615  max mem: 29171\n",
            "Epoch: [15]  [ 10/159]  eta: 0:12:48  lr: 0.000029  loss: 0.6481 (0.6829)  time: 5.1575  data: 4.6303  max mem: 29171\n",
            "Epoch: [15]  [ 20/159]  eta: 0:11:56  lr: 0.000029  loss: 0.6626 (0.6991)  time: 5.1559  data: 4.6280  max mem: 29171\n",
            "Epoch: [15]  [ 30/159]  eta: 0:11:00  lr: 0.000029  loss: 0.7224 (0.7009)  time: 5.1052  data: 4.5775  max mem: 29171\n",
            "Epoch: [15]  [ 40/159]  eta: 0:10:11  lr: 0.000029  loss: 0.6687 (0.6890)  time: 5.1142  data: 4.5876  max mem: 29171\n",
            "Epoch: [15]  [ 50/159]  eta: 0:09:21  lr: 0.000029  loss: 0.6035 (0.6773)  time: 5.1894  data: 4.6628  max mem: 29171\n",
            "Epoch: [15]  [ 60/159]  eta: 0:08:28  lr: 0.000029  loss: 0.6074 (0.6746)  time: 5.1400  data: 4.6127  max mem: 29171\n",
            "Epoch: [15]  [ 70/159]  eta: 0:07:37  lr: 0.000029  loss: 0.6555 (0.6800)  time: 5.1106  data: 4.5824  max mem: 29171\n",
            "Epoch: [15]  [ 80/159]  eta: 0:06:45  lr: 0.000029  loss: 0.6440 (0.6786)  time: 5.0996  data: 4.5719  max mem: 29171\n",
            "Epoch: [15]  [ 90/159]  eta: 0:05:52  lr: 0.000029  loss: 0.6605 (0.6808)  time: 5.0142  data: 4.4870  max mem: 29171\n",
            "Epoch: [15]  [100/159]  eta: 0:05:01  lr: 0.000029  loss: 0.7085 (0.6803)  time: 5.0420  data: 4.5144  max mem: 29171\n",
            "Epoch: [15]  [110/159]  eta: 0:04:10  lr: 0.000029  loss: 0.6336 (0.6752)  time: 5.0734  data: 4.5465  max mem: 29171\n",
            "Epoch: [15]  [120/159]  eta: 0:03:18  lr: 0.000029  loss: 0.6003 (0.6731)  time: 5.0253  data: 4.4994  max mem: 29171\n",
            "Epoch: [15]  [130/159]  eta: 0:02:27  lr: 0.000029  loss: 0.6246 (0.6723)  time: 5.0217  data: 4.4949  max mem: 29171\n",
            "Epoch: [15]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.6608 (0.6742)  time: 5.0733  data: 4.5464  max mem: 29171\n",
            "Epoch: [15]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6883 (0.6730)  time: 5.0966  data: 4.5689  max mem: 29171\n",
            "Epoch: [15]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.5719 (0.6681)  time: 5.0678  data: 4.5389  max mem: 29171\n",
            "Epoch: [15] Total time: 0:13:29 (5.0898 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.5719 (0.6681)\n",
            "Test:  [ 0/27]  eta: 0:03:32  loss: 0.0680 (0.0680)  acc1: 98.2222 (98.2222)  acc5: 99.5556 (99.5556)  time: 7.8801  data: 7.5764  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:03  loss: 0.2493 (0.2481)  acc1: 93.5556 (93.5758)  acc5: 99.1111 (98.7071)  time: 7.2916  data: 6.9874  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.3014 (0.2988)  acc1: 91.3333 (91.8624)  acc5: 98.2222 (98.4021)  time: 7.1518  data: 6.8476  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.3124 (0.2899)  acc1: 89.5556 (91.9833)  acc5: 97.7778 (98.5167)  time: 7.0344  data: 6.7346  max mem: 29171\n",
            "Test: Total time: 0:03:12 (7.1151 s / it)\n",
            "* Acc@1 91.983 Acc@5 98.517 loss 0.290\n",
            "Accuracy of the network on the 12000 test images: 92.0%\n",
            "Max accuracy: 91.98%\n",
            "Epoch: [16]  [  0/159]  eta: 0:13:29  lr: 0.000029  loss: 0.8043 (0.8043)  time: 5.0914  data: 4.5704  max mem: 29171\n",
            "Epoch: [16]  [ 10/159]  eta: 0:12:37  lr: 0.000029  loss: 0.6561 (0.6465)  time: 5.0832  data: 4.5565  max mem: 29171\n",
            "Epoch: [16]  [ 20/159]  eta: 0:11:47  lr: 0.000029  loss: 0.6602 (0.6809)  time: 5.0908  data: 4.5642  max mem: 29171\n",
            "Epoch: [16]  [ 30/159]  eta: 0:10:54  lr: 0.000029  loss: 0.6693 (0.6857)  time: 5.0666  data: 4.5406  max mem: 29171\n",
            "Epoch: [16]  [ 40/159]  eta: 0:10:01  lr: 0.000029  loss: 0.6693 (0.6840)  time: 5.0195  data: 4.4920  max mem: 29171\n",
            "Epoch: [16]  [ 50/159]  eta: 0:09:07  lr: 0.000029  loss: 0.6414 (0.6694)  time: 4.9578  data: 4.4302  max mem: 29171\n",
            "Epoch: [16]  [ 60/159]  eta: 0:08:17  lr: 0.000029  loss: 0.6243 (0.6629)  time: 4.9527  data: 4.4260  max mem: 29171\n",
            "Epoch: [16]  [ 70/159]  eta: 0:07:28  lr: 0.000029  loss: 0.6573 (0.6710)  time: 5.0613  data: 4.5351  max mem: 29171\n",
            "Epoch: [16]  [ 80/159]  eta: 0:06:38  lr: 0.000029  loss: 0.6573 (0.6659)  time: 5.0890  data: 4.5641  max mem: 29171\n",
            "Epoch: [16]  [ 90/159]  eta: 0:05:47  lr: 0.000029  loss: 0.6127 (0.6623)  time: 5.0177  data: 4.4920  max mem: 29171\n",
            "Epoch: [16]  [100/159]  eta: 0:04:57  lr: 0.000029  loss: 0.6243 (0.6635)  time: 5.0626  data: 4.5367  max mem: 29171\n",
            "Epoch: [16]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.6506 (0.6622)  time: 5.0815  data: 4.5553  max mem: 29171\n",
            "Epoch: [16]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 0.6664 (0.6642)  time: 5.0278  data: 4.5007  max mem: 29171\n",
            "Epoch: [16]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.6664 (0.6635)  time: 5.0184  data: 4.4925  max mem: 29171\n",
            "Epoch: [16]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.6282 (0.6625)  time: 5.0329  data: 4.5076  max mem: 29171\n",
            "Epoch: [16]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6340 (0.6627)  time: 5.0817  data: 4.5558  max mem: 29171\n",
            "Epoch: [16]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6340 (0.6604)  time: 5.0966  data: 4.5700  max mem: 29171\n",
            "Epoch: [16] Total time: 0:13:22 (5.0450 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6340 (0.6604)\n",
            "Test:  [ 0/27]  eta: 0:03:26  loss: 0.0644 (0.0644)  acc1: 98.8889 (98.8889)  acc5: 99.5556 (99.5556)  time: 7.6573  data: 7.3563  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:01  loss: 0.2395 (0.2443)  acc1: 93.7778 (93.8788)  acc5: 99.1111 (98.7475)  time: 7.1324  data: 6.8280  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.2987 (0.2948)  acc1: 91.3333 (92.0423)  acc5: 98.2222 (98.3915)  time: 7.0875  data: 6.7825  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.2994 (0.2849)  acc1: 90.0000 (92.1083)  acc5: 98.0000 (98.5250)  time: 7.0499  data: 6.7494  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0714 s / it)\n",
            "* Acc@1 92.108 Acc@5 98.525 loss 0.285\n",
            "Accuracy of the network on the 12000 test images: 92.1%\n",
            "Max accuracy: 92.11%\n",
            "Epoch: [17]  [  0/159]  eta: 0:13:57  lr: 0.000029  loss: 0.6172 (0.6172)  time: 5.2699  data: 4.7411  max mem: 29171\n",
            "Epoch: [17]  [ 10/159]  eta: 0:12:49  lr: 0.000029  loss: 0.6172 (0.6378)  time: 5.1675  data: 4.6399  max mem: 29171\n",
            "Epoch: [17]  [ 20/159]  eta: 0:11:55  lr: 0.000029  loss: 0.6298 (0.6539)  time: 5.1378  data: 4.6104  max mem: 29171\n",
            "Epoch: [17]  [ 30/159]  eta: 0:11:00  lr: 0.000029  loss: 0.6718 (0.6602)  time: 5.0883  data: 4.5609  max mem: 29171\n",
            "Epoch: [17]  [ 40/159]  eta: 0:10:05  lr: 0.000029  loss: 0.6718 (0.6653)  time: 5.0279  data: 4.5006  max mem: 29171\n",
            "Epoch: [17]  [ 50/159]  eta: 0:09:11  lr: 0.000029  loss: 0.6306 (0.6566)  time: 4.9701  data: 4.4426  max mem: 29171\n",
            "Epoch: [17]  [ 60/159]  eta: 0:08:19  lr: 0.000029  loss: 0.6188 (0.6552)  time: 4.9713  data: 4.4442  max mem: 29171\n",
            "Epoch: [17]  [ 70/159]  eta: 0:07:30  lr: 0.000029  loss: 0.6820 (0.6614)  time: 5.0549  data: 4.5281  max mem: 29171\n",
            "Epoch: [17]  [ 80/159]  eta: 0:06:39  lr: 0.000029  loss: 0.6801 (0.6580)  time: 5.0650  data: 4.5382  max mem: 29171\n",
            "Epoch: [17]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 0.6539 (0.6585)  time: 5.0057  data: 4.4788  max mem: 29171\n",
            "Epoch: [17]  [100/159]  eta: 0:04:58  lr: 0.000029  loss: 0.6501 (0.6568)  time: 5.0507  data: 4.5235  max mem: 29171\n",
            "Epoch: [17]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.6069 (0.6555)  time: 5.1081  data: 4.5815  max mem: 29171\n",
            "Epoch: [17]  [120/159]  eta: 0:03:17  lr: 0.000029  loss: 0.6193 (0.6538)  time: 5.1156  data: 4.5890  max mem: 29171\n",
            "Epoch: [17]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.6231 (0.6513)  time: 5.0594  data: 4.5323  max mem: 29171\n",
            "Epoch: [17]  [140/159]  eta: 0:01:36  lr: 0.000029  loss: 0.6452 (0.6551)  time: 5.0311  data: 4.5038  max mem: 29171\n",
            "Epoch: [17]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6798 (0.6561)  time: 5.0586  data: 4.5319  max mem: 29171\n",
            "Epoch: [17]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6204 (0.6537)  time: 5.0724  data: 4.5455  max mem: 29171\n",
            "Epoch: [17] Total time: 0:13:24 (5.0592 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6204 (0.6537)\n",
            "Test:  [ 0/27]  eta: 0:03:24  loss: 0.0645 (0.0645)  acc1: 98.8889 (98.8889)  acc5: 99.3333 (99.3333)  time: 7.5600  data: 7.2582  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.2412 (0.2407)  acc1: 93.5556 (93.8384)  acc5: 99.1111 (98.7475)  time: 7.0801  data: 6.7775  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.2907 (0.2897)  acc1: 91.3333 (92.0952)  acc5: 98.2222 (98.4444)  time: 7.0308  data: 6.7277  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.2969 (0.2812)  acc1: 90.4445 (92.1750)  acc5: 98.0000 (98.5833)  time: 7.0314  data: 6.7321  max mem: 29171\n",
            "Test: Total time: 0:03:10 (7.0389 s / it)\n",
            "* Acc@1 92.175 Acc@5 98.583 loss 0.281\n",
            "Accuracy of the network on the 12000 test images: 92.2%\n",
            "Max accuracy: 92.18%\n",
            "Epoch: [18]  [  0/159]  eta: 0:13:36  lr: 0.000029  loss: 0.7728 (0.7728)  time: 5.1364  data: 4.6105  max mem: 29171\n",
            "Epoch: [18]  [ 10/159]  eta: 0:12:38  lr: 0.000029  loss: 0.6508 (0.6375)  time: 5.0911  data: 4.5630  max mem: 29171\n",
            "Epoch: [18]  [ 20/159]  eta: 0:11:46  lr: 0.000029  loss: 0.6429 (0.6591)  time: 5.0790  data: 4.5516  max mem: 29171\n",
            "Epoch: [18]  [ 30/159]  eta: 0:10:56  lr: 0.000029  loss: 0.6429 (0.6594)  time: 5.0935  data: 4.5666  max mem: 29171\n",
            "Epoch: [18]  [ 40/159]  eta: 0:10:03  lr: 0.000029  loss: 0.6827 (0.6632)  time: 5.0557  data: 4.5298  max mem: 29171\n",
            "Epoch: [18]  [ 50/159]  eta: 0:09:09  lr: 0.000029  loss: 0.6295 (0.6502)  time: 4.9550  data: 4.4295  max mem: 29171\n",
            "Epoch: [18]  [ 60/159]  eta: 0:08:19  lr: 0.000029  loss: 0.6185 (0.6456)  time: 4.9872  data: 4.4592  max mem: 29171\n",
            "Epoch: [18]  [ 70/159]  eta: 0:07:30  lr: 0.000029  loss: 0.6427 (0.6476)  time: 5.1162  data: 4.5871  max mem: 29171\n",
            "Epoch: [18]  [ 80/159]  eta: 0:06:39  lr: 0.000029  loss: 0.6429 (0.6449)  time: 5.1053  data: 4.5778  max mem: 29171\n",
            "Epoch: [18]  [ 90/159]  eta: 0:05:48  lr: 0.000029  loss: 0.5947 (0.6417)  time: 5.0014  data: 4.4757  max mem: 29171\n",
            "Epoch: [18]  [100/159]  eta: 0:04:58  lr: 0.000029  loss: 0.6252 (0.6399)  time: 5.0449  data: 4.5198  max mem: 29171\n",
            "Epoch: [18]  [110/159]  eta: 0:04:07  lr: 0.000029  loss: 0.6268 (0.6365)  time: 5.0854  data: 4.5595  max mem: 29171\n",
            "Epoch: [18]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 0.6309 (0.6353)  time: 5.0114  data: 4.4843  max mem: 29171\n",
            "Epoch: [18]  [130/159]  eta: 0:02:26  lr: 0.000029  loss: 0.6165 (0.6362)  time: 4.9615  data: 4.4347  max mem: 29171\n",
            "Epoch: [18]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.6735 (0.6410)  time: 4.9715  data: 4.4462  max mem: 29171\n",
            "Epoch: [18]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6735 (0.6395)  time: 5.0161  data: 4.4917  max mem: 29171\n",
            "Epoch: [18]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6013 (0.6382)  time: 5.0392  data: 4.5140  max mem: 29171\n",
            "Epoch: [18] Total time: 0:13:20 (5.0361 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6013 (0.6382)\n",
            "Test:  [ 0/27]  eta: 0:03:18  loss: 0.0605 (0.0605)  acc1: 98.2222 (98.2222)  acc5: 99.5556 (99.5556)  time: 7.3680  data: 7.0659  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.2301 (0.2352)  acc1: 94.4445 (93.8586)  acc5: 98.8889 (98.7879)  time: 7.0767  data: 6.7736  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.2806 (0.2865)  acc1: 90.8889 (92.1058)  acc5: 98.4445 (98.5185)  time: 7.0541  data: 6.7508  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.2888 (0.2762)  acc1: 90.0000 (92.2917)  acc5: 98.4445 (98.6250)  time: 6.9979  data: 6.6988  max mem: 29171\n",
            "Test: Total time: 0:03:09 (7.0127 s / it)\n",
            "* Acc@1 92.292 Acc@5 98.625 loss 0.276\n",
            "Accuracy of the network on the 12000 test images: 92.3%\n",
            "Max accuracy: 92.29%\n",
            "Epoch: [19]  [  0/159]  eta: 0:13:16  lr: 0.000029  loss: 0.6173 (0.6173)  time: 5.0065  data: 4.4850  max mem: 29171\n",
            "Epoch: [19]  [ 10/159]  eta: 0:12:33  lr: 0.000029  loss: 0.5863 (0.6085)  time: 5.0584  data: 4.5306  max mem: 29171\n",
            "Epoch: [19]  [ 20/159]  eta: 0:11:44  lr: 0.000029  loss: 0.5795 (0.6302)  time: 5.0723  data: 4.5446  max mem: 29171\n",
            "Epoch: [19]  [ 30/159]  eta: 0:10:52  lr: 0.000029  loss: 0.5908 (0.6256)  time: 5.0597  data: 4.5258  max mem: 29171\n",
            "Epoch: [19]  [ 40/159]  eta: 0:09:59  lr: 0.000029  loss: 0.6110 (0.6262)  time: 4.9991  data: 4.4657  max mem: 29171\n",
            "Epoch: [19]  [ 50/159]  eta: 0:09:06  lr: 0.000029  loss: 0.6012 (0.6205)  time: 4.9533  data: 4.4271  max mem: 29171\n",
            "Epoch: [19]  [ 60/159]  eta: 0:08:15  lr: 0.000029  loss: 0.6012 (0.6189)  time: 4.9437  data: 4.4179  max mem: 29171\n",
            "Epoch: [19]  [ 70/159]  eta: 0:07:26  lr: 0.000029  loss: 0.6211 (0.6200)  time: 5.0071  data: 4.4823  max mem: 29171\n",
            "Epoch: [19]  [ 80/159]  eta: 0:06:35  lr: 0.000029  loss: 0.6136 (0.6207)  time: 5.0269  data: 4.5016  max mem: 29171\n",
            "Epoch: [19]  [ 90/159]  eta: 0:05:45  lr: 0.000029  loss: 0.6136 (0.6205)  time: 4.9497  data: 4.4232  max mem: 29171\n",
            "Epoch: [19]  [100/159]  eta: 0:04:55  lr: 0.000029  loss: 0.6114 (0.6166)  time: 4.9894  data: 4.4618  max mem: 29171\n",
            "Epoch: [19]  [110/159]  eta: 0:04:05  lr: 0.000029  loss: 0.6172 (0.6188)  time: 5.0391  data: 4.5114  max mem: 29171\n",
            "Epoch: [19]  [120/159]  eta: 0:03:15  lr: 0.000029  loss: 0.6265 (0.6191)  time: 5.0166  data: 4.4904  max mem: 29171\n",
            "Epoch: [19]  [130/159]  eta: 0:02:25  lr: 0.000029  loss: 0.5812 (0.6188)  time: 5.0092  data: 4.4830  max mem: 29171\n",
            "Epoch: [19]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.6008 (0.6222)  time: 5.0289  data: 4.5033  max mem: 29171\n",
            "Epoch: [19]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6636 (0.6247)  time: 5.0472  data: 4.5220  max mem: 29171\n",
            "Epoch: [19]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6231 (0.6242)  time: 5.0414  data: 4.5160  max mem: 29171\n",
            "Epoch: [19] Total time: 0:13:17 (5.0131 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6231 (0.6242)\n",
            "Test:  [ 0/27]  eta: 0:03:22  loss: 0.0588 (0.0588)  acc1: 98.4445 (98.4445)  acc5: 99.5556 (99.5556)  time: 7.4913  data: 7.1877  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.2331 (0.2330)  acc1: 93.7778 (93.7980)  acc5: 99.1111 (98.7879)  time: 7.1056  data: 6.8017  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.2818 (0.2841)  acc1: 91.3333 (92.1164)  acc5: 98.0000 (98.5185)  time: 7.0589  data: 6.7555  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.2839 (0.2745)  acc1: 90.4445 (92.2917)  acc5: 98.0000 (98.6333)  time: 7.0030  data: 6.7043  max mem: 29171\n",
            "Test: Total time: 0:03:09 (7.0299 s / it)\n",
            "* Acc@1 92.292 Acc@5 98.633 loss 0.274\n",
            "Accuracy of the network on the 12000 test images: 92.3%\n",
            "Max accuracy: 92.29%\n",
            "Epoch: [20]  [  0/159]  eta: 0:13:26  lr: 0.000029  loss: 0.6837 (0.6837)  time: 5.0751  data: 4.5480  max mem: 29171\n",
            "Epoch: [20]  [ 10/159]  eta: 0:12:42  lr: 0.000029  loss: 0.5673 (0.5967)  time: 5.1154  data: 4.5895  max mem: 29171\n",
            "Epoch: [20]  [ 20/159]  eta: 0:11:50  lr: 0.000029  loss: 0.5673 (0.6145)  time: 5.1117  data: 4.5859  max mem: 29171\n",
            "Epoch: [20]  [ 30/159]  eta: 0:10:55  lr: 0.000029  loss: 0.6194 (0.6155)  time: 5.0658  data: 4.5389  max mem: 29171\n",
            "Epoch: [20]  [ 40/159]  eta: 0:10:02  lr: 0.000029  loss: 0.6363 (0.6231)  time: 5.0084  data: 4.4806  max mem: 29171\n",
            "Epoch: [20]  [ 50/159]  eta: 0:09:08  lr: 0.000029  loss: 0.6298 (0.6138)  time: 4.9451  data: 4.4184  max mem: 29171\n",
            "Epoch: [20]  [ 60/159]  eta: 0:08:17  lr: 0.000029  loss: 0.5915 (0.6129)  time: 4.9563  data: 4.4227  max mem: 29171\n",
            "Epoch: [20]  [ 70/159]  eta: 0:07:27  lr: 0.000029  loss: 0.6003 (0.6154)  time: 5.0386  data: 4.5063  max mem: 29171\n",
            "Epoch: [20]  [ 80/159]  eta: 0:06:36  lr: 0.000029  loss: 0.6098 (0.6129)  time: 5.0057  data: 4.4806  max mem: 29171\n",
            "Epoch: [20]  [ 90/159]  eta: 0:05:45  lr: 0.000029  loss: 0.6192 (0.6132)  time: 4.9400  data: 4.4147  max mem: 29171\n",
            "Epoch: [20]  [100/159]  eta: 0:04:56  lr: 0.000029  loss: 0.5783 (0.6101)  time: 5.0262  data: 4.5016  max mem: 29171\n",
            "Epoch: [20]  [110/159]  eta: 0:04:05  lr: 0.000029  loss: 0.5566 (0.6096)  time: 5.0456  data: 4.5208  max mem: 29171\n",
            "Epoch: [20]  [120/159]  eta: 0:03:15  lr: 0.000029  loss: 0.5971 (0.6111)  time: 4.9817  data: 4.4570  max mem: 29171\n",
            "Epoch: [20]  [130/159]  eta: 0:02:25  lr: 0.000029  loss: 0.5699 (0.6084)  time: 4.9654  data: 4.4397  max mem: 29171\n",
            "Epoch: [20]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.5734 (0.6102)  time: 4.9635  data: 4.4382  max mem: 29171\n",
            "Epoch: [20]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6017 (0.6098)  time: 4.9858  data: 4.4617  max mem: 29171\n",
            "Epoch: [20]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.6017 (0.6084)  time: 5.0083  data: 4.4835  max mem: 29171\n",
            "Epoch: [20] Total time: 0:13:16 (5.0088 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.6017 (0.6084)\n",
            "Test:  [ 0/27]  eta: 0:03:22  loss: 0.0572 (0.0572)  acc1: 99.1111 (99.1111)  acc5: 99.5556 (99.5556)  time: 7.5182  data: 7.2129  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.2329 (0.2322)  acc1: 94.4445 (94.0000)  acc5: 98.8889 (98.7677)  time: 7.0844  data: 6.7793  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.2819 (0.2812)  acc1: 91.3333 (92.1693)  acc5: 98.4445 (98.5079)  time: 7.0350  data: 6.7303  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.2866 (0.2711)  acc1: 90.0000 (92.3417)  acc5: 98.4445 (98.6500)  time: 6.9778  data: 6.6780  max mem: 29171\n",
            "Test: Total time: 0:03:09 (7.0037 s / it)\n",
            "* Acc@1 92.342 Acc@5 98.650 loss 0.271\n",
            "Accuracy of the network on the 12000 test images: 92.3%\n",
            "Max accuracy: 92.34%\n",
            "Epoch: [21]  [  0/159]  eta: 0:13:37  lr: 0.000029  loss: 0.6362 (0.6362)  time: 5.1427  data: 4.6212  max mem: 29171\n",
            "Epoch: [21]  [ 10/159]  eta: 0:12:34  lr: 0.000029  loss: 0.5600 (0.5737)  time: 5.0643  data: 4.5383  max mem: 29171\n",
            "Epoch: [21]  [ 20/159]  eta: 0:11:42  lr: 0.000029  loss: 0.5600 (0.5921)  time: 5.0533  data: 4.5264  max mem: 29171\n",
            "Epoch: [21]  [ 30/159]  eta: 0:10:51  lr: 0.000029  loss: 0.5905 (0.6083)  time: 5.0371  data: 4.5099  max mem: 29171\n",
            "Epoch: [21]  [ 40/159]  eta: 0:10:00  lr: 0.000029  loss: 0.6289 (0.6109)  time: 5.0263  data: 4.4991  max mem: 29171\n",
            "Epoch: [21]  [ 50/159]  eta: 0:09:07  lr: 0.000029  loss: 0.6289 (0.6102)  time: 4.9966  data: 4.4690  max mem: 29171\n",
            "Epoch: [21]  [ 60/159]  eta: 0:08:16  lr: 0.000029  loss: 0.5637 (0.6072)  time: 4.9703  data: 4.4427  max mem: 29171\n",
            "Epoch: [21]  [ 70/159]  eta: 0:07:27  lr: 0.000029  loss: 0.5739 (0.6091)  time: 5.0364  data: 4.5106  max mem: 29171\n",
            "Epoch: [21]  [ 80/159]  eta: 0:06:37  lr: 0.000029  loss: 0.5739 (0.6049)  time: 5.0561  data: 4.5304  max mem: 29171\n",
            "Epoch: [21]  [ 90/159]  eta: 0:05:46  lr: 0.000029  loss: 0.5792 (0.6044)  time: 4.9761  data: 4.4501  max mem: 29171\n",
            "Epoch: [21]  [100/159]  eta: 0:04:56  lr: 0.000029  loss: 0.5887 (0.6023)  time: 5.0010  data: 4.4757  max mem: 29171\n",
            "Epoch: [21]  [110/159]  eta: 0:04:06  lr: 0.000029  loss: 0.5855 (0.6003)  time: 5.0430  data: 4.5095  max mem: 29171\n",
            "Epoch: [21]  [120/159]  eta: 0:03:15  lr: 0.000029  loss: 0.5704 (0.5996)  time: 5.0233  data: 4.4891  max mem: 29171\n",
            "Epoch: [21]  [130/159]  eta: 0:02:25  lr: 0.000029  loss: 0.5745 (0.6009)  time: 5.0106  data: 4.4841  max mem: 29171\n",
            "Epoch: [21]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.5952 (0.6031)  time: 5.0037  data: 4.4776  max mem: 29171\n",
            "Epoch: [21]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6179 (0.6048)  time: 5.0609  data: 4.5351  max mem: 29171\n",
            "Epoch: [21]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.5816 (0.6040)  time: 5.0708  data: 4.5453  max mem: 29171\n",
            "Epoch: [21] Total time: 0:13:19 (5.0261 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.5816 (0.6040)\n",
            "Test:  [ 0/27]  eta: 0:03:24  loss: 0.0543 (0.0543)  acc1: 98.8889 (98.8889)  acc5: 99.7778 (99.7778)  time: 7.5581  data: 7.2548  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:00  loss: 0.2243 (0.2270)  acc1: 94.0000 (94.0404)  acc5: 99.3333 (98.8687)  time: 7.0793  data: 6.7747  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:49  loss: 0.2814 (0.2792)  acc1: 91.5556 (92.2011)  acc5: 98.0000 (98.5397)  time: 7.0291  data: 6.7250  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:06  loss: 0.2839 (0.2699)  acc1: 90.4445 (92.3500)  acc5: 98.0000 (98.6417)  time: 6.9632  data: 6.6641  max mem: 29171\n",
            "Test: Total time: 0:03:08 (6.9949 s / it)\n",
            "* Acc@1 92.350 Acc@5 98.642 loss 0.270\n",
            "Accuracy of the network on the 12000 test images: 92.4%\n",
            "Max accuracy: 92.35%\n",
            "Epoch: [22]  [  0/159]  eta: 0:14:02  lr: 0.000029  loss: 0.6138 (0.6138)  time: 5.2982  data: 4.7722  max mem: 29171\n",
            "Epoch: [22]  [ 10/159]  eta: 0:12:43  lr: 0.000029  loss: 0.5886 (0.5698)  time: 5.1230  data: 4.5928  max mem: 29171\n",
            "Epoch: [22]  [ 20/159]  eta: 0:11:48  lr: 0.000029  loss: 0.5886 (0.5945)  time: 5.0880  data: 4.5586  max mem: 29171\n",
            "Epoch: [22]  [ 30/159]  eta: 0:10:55  lr: 0.000029  loss: 0.6063 (0.5960)  time: 5.0615  data: 4.5349  max mem: 29171\n",
            "Epoch: [22]  [ 40/159]  eta: 0:10:01  lr: 0.000029  loss: 0.6063 (0.6032)  time: 5.0062  data: 4.4806  max mem: 29171\n",
            "Epoch: [22]  [ 50/159]  eta: 0:09:07  lr: 0.000029  loss: 0.5950 (0.5980)  time: 4.9223  data: 4.3959  max mem: 29171\n",
            "Epoch: [22]  [ 60/159]  eta: 0:08:16  lr: 0.000029  loss: 0.5510 (0.5885)  time: 4.9219  data: 4.3955  max mem: 29171\n",
            "Epoch: [22]  [ 70/159]  eta: 0:07:26  lr: 0.000029  loss: 0.5510 (0.5981)  time: 5.0123  data: 4.4857  max mem: 29171\n",
            "Epoch: [22]  [ 80/159]  eta: 0:06:36  lr: 0.000029  loss: 0.5597 (0.5948)  time: 5.0412  data: 4.5152  max mem: 29171\n",
            "Epoch: [22]  [ 90/159]  eta: 0:05:45  lr: 0.000029  loss: 0.5854 (0.5952)  time: 4.9868  data: 4.4614  max mem: 29171\n",
            "Epoch: [22]  [100/159]  eta: 0:04:56  lr: 0.000029  loss: 0.5731 (0.5933)  time: 5.0316  data: 4.5060  max mem: 29171\n",
            "Epoch: [22]  [110/159]  eta: 0:04:06  lr: 0.000029  loss: 0.5702 (0.5947)  time: 5.0889  data: 4.5637  max mem: 29171\n",
            "Epoch: [22]  [120/159]  eta: 0:03:16  lr: 0.000029  loss: 0.6000 (0.5965)  time: 5.0600  data: 4.5351  max mem: 29171\n",
            "Epoch: [22]  [130/159]  eta: 0:02:25  lr: 0.000029  loss: 0.5829 (0.5935)  time: 5.0378  data: 4.5116  max mem: 29171\n",
            "Epoch: [22]  [140/159]  eta: 0:01:35  lr: 0.000029  loss: 0.5628 (0.5948)  time: 5.0443  data: 4.5180  max mem: 29171\n",
            "Epoch: [22]  [150/159]  eta: 0:00:45  lr: 0.000029  loss: 0.6159 (0.5971)  time: 5.0643  data: 4.5382  max mem: 29171\n",
            "Epoch: [22]  [158/159]  eta: 0:00:05  lr: 0.000029  loss: 0.5514 (0.5937)  time: 5.0517  data: 4.5241  max mem: 29171\n",
            "Epoch: [22] Total time: 0:13:19 (5.0312 s / it)\n",
            "Averaged stats: lr: 0.000029  loss: 0.5514 (0.5937)\n",
            "Test:  [ 0/27]  eta: 0:03:19  loss: 0.0513 (0.0513)  acc1: 98.6667 (98.6667)  acc5: 99.7778 (99.7778)  time: 7.4033  data: 7.0970  max mem: 29171\n",
            "Test:  [10/27]  eta: 0:02:02  loss: 0.2246 (0.2256)  acc1: 94.0000 (93.8788)  acc5: 99.1111 (98.9697)  time: 7.2317  data: 6.9265  max mem: 29171\n",
            "Test:  [20/27]  eta: 0:00:50  loss: 0.2694 (0.2760)  acc1: 91.7778 (92.3598)  acc5: 98.4445 (98.6349)  time: 7.1551  data: 6.8498  max mem: 29171\n",
            "Test:  [26/27]  eta: 0:00:07  loss: 0.2790 (0.2668)  acc1: 90.2222 (92.5250)  acc5: 98.4445 (98.6917)  time: 7.1178  data: 6.8167  max mem: 29171\n",
            "Test: Total time: 0:03:12 (7.1242 s / it)\n",
            "* Acc@1 92.525 Acc@5 98.692 loss 0.267\n",
            "Accuracy of the network on the 12000 test images: 92.5%\n",
            "Max accuracy: 92.53%\n",
            "Epoch: [23]  [  0/159]  eta: 0:13:32  lr: 0.000029  loss: 0.5823 (0.5823)  time: 5.1112  data: 4.5818  max mem: 29171\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-966154685a25>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_wdm_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_linear_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtest_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-95317eee6636>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         train_stats = train_one_epoch(\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ViT_Training/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, loss_scaler, max_norm, model_ema, mixup_fn, set_training_mode, args)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ViT_Training/utils.py\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mlog_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser('DeiT training and evaluation script', parents=[get_args_parser()])\n",
        "    args = parser.parse_args(args=[])\n",
        "    if args.output_dir:\n",
        "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    args.num_workers = 0\n",
        "    args.pin_mem = True\n",
        "    args.eval = False\n",
        "    args.data_path = '/content/drive/MyDrive/mini_imagenet'\n",
        "\n",
        "    #args.model = 'deit_tiny_patch16_224'\n",
        "    #args.resume = '/content/ViT_Training/resumed_ckpt/deit_tiny_patch16_224-a1311bcf.pth'\n",
        "\n",
        "    args.model = 'deit_tiny_patch16_224_quant'\n",
        "    #args.resume = './resumed_ckpt/best_checkpoint.pth'\n",
        "    #args.resume = '/content/drive/MyDrive/mini_imagenet/resumed_ckpt/best_checkpoint_int8.pth'\n",
        "    args.finetune = '/content/drive/MyDrive/mini_imagenet/resumed_ckpt/best_checkpoint_int8.pth'\n",
        "\n",
        "    args.output_dir = '/content/drive/MyDrive/mini_imagenet/finetune'\n",
        "\n",
        "    args.batch_size = 300\n",
        "    args.drop_path = 0\n",
        "    args.lr = 5e-5\n",
        "    args.min_lr = 0\n",
        "    args.epochs = 300\n",
        "    args.warmup_epochs = 0\n",
        "    args.weight_decay = 1e-8\n",
        "    args.wbits = 8\n",
        "    args.abits = 8\n",
        "    args.dist_eval = 0\n",
        "    args.headwise = True\n",
        "    args.input_noise_std = 0.0 #0.03\n",
        "    #args.input_noise_std_w = 0.3\n",
        "    args.output_noise_std = 0.0 #0.05\n",
        "    args.phase_noise_std = 0\n",
        "    args.num_wavelength = 1\n",
        "    args.channel_spacing = 0.4\n",
        "    args.seed = 0\n",
        "    args.enable_wdm_noise = False\n",
        "    args.enable_linear_noise = True\n",
        "    test_stats = main(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "pKCtOvnjk-WB"
      },
      "id": "pKCtOvnjk-WB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}